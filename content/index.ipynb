{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_output",
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "build = 'archive' # 'archive' uses the neurolibre archive of the data., 'latest' would download the latest versions of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<head>\n",
    "\n",
    "<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
    "<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=STIX+Two+Text:ital,wght@0,400;0,700;1,400&display=swap\" rel=\"stylesheet\">\n",
    "</head>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<center>\n",
    "<b>\n",
    "<h3>\n",
    "Paper is not enough: Crowdsourcing the T<sub>1</sub> mapping common ground via the ISMRM reproducibility challenge\n",
    "</h3>\n",
    "\n",
    "<p>\n",
    "<sup>*</sup>Mathieu Boudreau<sup>1,2</sup>, <sup>*</sup>Agah Karakuzu<sup>1</sup>, Julien Cohen-Adad<sup>1,3,4,5</sup>, Ecem Bozkurt<sup>6</sup>, Madeline Carr<sup>7,8</sup>, Marco Castellaro<sup>9</sup>, Luis Concha<sup>10</sup>, Mariya Doneva<sup>11</sup>, Seraina A. Dual<sup>12</sup>, Alex Ensworth<sup>13,14</sup>, Alexandru Foias<sup>1</sup>, Véronique Fortier<sup>15,16</sup>, Refaat E. Gabr<sup>17</sup>, Guillaume Gilbert<sup>18</sup>, Carri K. Glide-Hurst<sup>19</sup>, Matthew Grech-Sollars<sup>20,21</sup>, Siyuan Hu<sup>22</sup>, Oscar Jalnefjord<sup>23,24</sup>, Jorge Jovicich<sup>25</sup>, Kübra Keskin<sup>6</sup>, Peter Koken<sup>11</sup>, Anastasia Kolokotronis<sup>13,26</sup>, Simran Kukran<sup>27,28</sup>, Nam. G. Lee<sup>6</sup>, Ives R. Levesque<sup>13,29</sup>, Bochao Li<sup>6</sup>, Dan Ma<sup>22</sup>, Burkhard Mädler<sup>30</sup>, Nyasha Maforo<sup>31,32</sup>, Jamie Near<sup>33,34</sup>, Erick Pasaye<sup>10</sup>, Alonso Ramirez-Manzanares<sup>35</sup>, Ben Statton<sup>36</sup>,Christian Stehning<sup>30</sup>, Stefano Tambalo<sup>25</sup>, Ye Tian<sup>6</sup>, Chenyang Wang<sup>37</sup>, Kilian Weiss<sup>30</sup>, Niloufar Zakariaei<sup>38</sup>, Shuo Zhang<sup>30</sup>, Ziwei Zhao<sup>6</sup>, Nikola Stikov<sup>1,2,39</sup>\n",
    "</p>\n",
    "</b>\n",
    "\n",
    "<ul style=\"list-style-type: none\">\n",
    "<li><sup>*</sup>Authors MB and AK contributed equally to this work</li>\n",
    "</ul>\n",
    "</center>\n",
    "\n",
    "<p style=\"text-align:justify;font-size:70%\">\n",
    "<sup>1</sup>NeuroPoly Lab, Polytechnique Montréal, Montreal, Quebec, Canada,\n",
    "<sup>2</sup>Montreal Heart Institute, Montreal, Quebec, Canada,\n",
    "<sup>3</sup>Unité de Neuroimagerie Fonctionnelle (UNF), Centre de recherche de l’Institut Universitaire de Gériatrie de Montréal (CRIUGM), Montreal, Quebec, Canada,\n",
    "<sup>4</sup>Mila - Quebec AI Institute, Montreal, QC, Canada,\n",
    "<sup>5</sup>Centre de recherche du CHU Sainte-Justine, Université de Montréal, Montreal, QC, Canada,\n",
    "<sup>6</sup>Magnetic Resonance Engineering Laboratory (MREL), University of Southern California, Los Angeles, California, USA,\n",
    "<sup>7</sup>Medical Physics, Ingham Institute for Applied Medical Research, Liverpool, Australia,\n",
    "<sup>8</sup>Department of Medical Physics, Liverpool and Macarthur Cancer Therapy Centres, Liverpool, Australia,\n",
    "<sup>9</sup>Department of Information Engineering, University of Padova, Padova, Italy,\n",
    "<sup>10</sup>Institute of Neurobiology, Universidad Nacional Autónoma de México Campus Juriquilla, Querétaro, México,\n",
    "<sup>11</sup>Philips Research Hamburg, Germany,\n",
    "<sup>12</sup>Department of Radiology, Stanford University, Stanford, California, United States,\n",
    "<sup>13</sup>Medical Physics Unit, McGill University, Montreal, Canada,\n",
    "<sup>14</sup>University of British Columbia, Vancouver, Canada,\n",
    "<sup>15</sup>Department of Medical Imaging, McGill University Health Centre, Montreal, Quebec, Canada\n",
    "<sup>16</sup>Department of Radiology, McGill University, Montreal, Quebec, Canada,\n",
    "<sup>17</sup>Department of Diagnostic and Interventional Imaging, University of Texas Health Science Center at Houston, McGovern Medical School, Houston, Texas, USA, \n",
    "<sup>18</sup>MR Clinical Science, Philips Canada, Mississauga, Ontario, Canada,\n",
    "<sup>19</sup>Department of Human Oncology, University of Wisconsin-Madison, Madison, Wisconsin, USA,\n",
    "<sup>20</sup>Centre for Medical Image Computing, Department of Computer Science, University College London, London, UK,\n",
    "<sup>21</sup>Lysholm Department of Neuroradiology, National Hospital for Neurology and Neurosurgery, University College London Hospitals NHS Foundation Trust, London, UK,\n",
    "<sup>22</sup>Department of Biomedical Engineering, Case Western Reserve University, Cleveland, Ohio, USA, \n",
    "<sup>23</sup>Department of Medical Radiation Sciences, Institute of Clinical Sciences, Sahlgrenska Academy, University of Gothenburg, Gothenburg, Sweden,\n",
    "<sup>24</sup>Biomedical Engineering, Sahlgrenska University Hospital, Gothenburg, Sweden, \n",
    "<sup>25</sup>Center for Mind/Brain Sciences, University of Trento, Italy,\n",
    "<sup>26</sup>Hopital Maisonneuve-Rosemont, Montreal, Canada,\n",
    "<sup>27</sup>Bioengineering, Imperial College London, UK,\n",
    "<sup>28</sup>Radiotherapy and Imaging, Insitute of Cancer Research, Imperial College London, UK,\n",
    "<sup>29</sup>Research Institute of the McGill University Health Centre, Montreal, Canada,\n",
    "<sup>30</sup>Clinical Science, Philips Healthcare, Germany,\n",
    "<sup>31</sup>Department of Radiological Sciences, University of California Los Angeles, Los Angeles, CA, USA,\n",
    "<sup>32</sup>Physics and Biology in Medicine IDP, University of California Los Angeles, Los Angeles, CA, USA,\n",
    "<sup>33</sup>Douglas Brain Imaging Centre, Montreal, Canada,\n",
    "<sup>34</sup>Sunnybrook Research Institute, Toronto, Canada,\n",
    "<sup>35</sup>Computer Science Department, Centro de Investigación en Matemáticas, A.C., Guanajuato, México,\n",
    "<sup>36</sup>Medical Research Council, London Institute of Medical Sciences, Imperial College London, London, United Kingdom,\n",
    "<sup>37</sup>Department of Radiation Oncology - CNS Service, The University of Texas MD Anderson Cancer Center, Texas, USA,\n",
    "<sup>38</sup>Department of Biomedical Engineering, University of British Columbia, British Columbia, Canada,\n",
    "<sup>39</sup>Center for Advanced Interdisciplinary Research, Ss. Cyril and Methodius University, Skopje, North Macedonia\n",
    "</p>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "\n",
    "\n",
    "**Purpose:** T<sub>1</sub> mapping is a widely used quantitative MRI technique, but its tissue-specific values remain inconsistent across protocols, sites, and vendors. The ISMRM Reproducible Research and Quantitative MR study groups jointly launched a challenge to assess the reproducibility of a well-established inversion recovery T<sub>1</sub> mapping technique, with acquisition details published solely as a PDF, on a standardized phantom and in human brains.\n",
    "\n",
    "**Methods:** The challenge used the acquisition protocol from Barral et al. 2010. Researchers collected T<sub>1</sub> mapping data on the ISMRM/NIST phantom and/or in human brains. Data submission, pipeline development, and analysis were conducted using open-source platforms. Inter-submission and intra-submission comparisons were performed.\n",
    "\n",
    "**Results:** Eighteen submissions (39 phantom and 56 human datasets) on scanners by three MRI vendors were collected at 3T (except one, at 0.35T). The mean coefficient of variation (COV) was 6.1% for inter-submission phantom measurements, and 2.9% for intra-submission measurements. For humans, inter-/intra-submission COV was 5.9/3.2% in the genu and 16/6.9% in the cortex. An interactive dashboard for data visualization was also developed: <a href = \"https://rrsg2020.dashboards.neurolibre.org\">https://rrsg2020.dashboards.neurolibre.org</a>.\n",
    "\n",
    "**Conclusion:** The T<sub>1</sub> inter-submission variability was twice as high as the intra-submission variability in both phantoms and human brains, indicating that the acquisition details in the selected paper were insufficient to reproduce a quantitative MRI protocol. This study reports the inherent uncertainty in T<sub>1</sub> measures across independent research groups, bringing us one step closer to a practical clinical baseline of T<sub>1</sub> variations in vivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"display:inline\"><center> Dashboard: Challenge Submissions </center></h2>\n",
    "<iframe src=\"https://rrsg2020.dashboards.neurolibre.org\" width=\"120%\" height=\"750px\" style=\"border:none;margin: 0 -10%\"></iframe>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 &nbsp; &nbsp; | &nbsp; &nbsp; INTRODUCTION\n",
    "\n",
    "\n",
    "Significant challenges exist in the reproducibility of quantitative MRI (qMRI) {cite:p}`Keenan2019-ni`. Despite its promise of improving the specificity and reproducibility of MRI acquisitions, few qMRI techniques have been integrated into clinical practice. Even the most fundamental MR parameters cannot be measured with sufficient reproducibility and precision across clinical scanners to pass the second of six stages of technical assessment for clinical biomarkers {cite:p}`Fryback1991-sy,Schweitzer2016-fl,Seiberlich2020-xe`. Half a century has passed since the first quantitative T<sub>1</sub> (spin-lattice relaxation time) measurements were first reported as a potential biomarker for tumors {cite:p}`Damadian1971-sc`, followed shortly thereafter by the first in vivo T<sub>1</sub> maps {cite:p}`Pykett1978-mk` of tumors, but there is still disagreement in reported values for this fundamental parameter across different sites, vendors, and measurement techniques {cite:p}`Stikov2015-qj`.\n",
    "\n",
    "\n",
    "Among fundamental MRI parameters, T<sub>1</sub> holds significant importance {cite:p}`Boudreau2020-jf`. T<sub>1</sub> represents the time constant for recovery of equilibrium longitudinal magnetization. T<sub>1</sub> values will vary depending on the molecular mobility and magnetic field strength {cite:p}`Bottomley1984-qx,Wansapura1999-tf,Dieringer2014-qz`. Knowledge of the T<sub>1</sub> values for tissue is crucial for optimizing clinical MRI sequences for contrast and time efficiency {cite:p}`Ernst1966-pp,Redpath1994-sb,Tofts1997-ln` and to calibrate other quantitative MRI techniques {cite:p}`Sled2001-fz,Yuan2012-xh`. Inversion recovery (IR) {cite:p}`Drain1949-yk,Hahn1949-wf` is considered the gold standard for T<sub>1</sub> measurement due to its robustness against effects like B<sub>1</sub> inhomogeneity {cite:p}`Stikov2015-qj`, but its long acquisition times limit the clinical use of IR for T<sub>1</sub> mapping {cite:p}`Stikov2015-qj`. In practice, IR is often used as a reference for validating other T<sub>1</sub> mapping techniques, such as variable flip angle imaging (VFA) {cite:p}`Fram1987-jj,Deoni2003-qc,Cheng2006-qe`, Look-Locker {cite:p}`Look1970-no,Messroghli2004-iv,Piechnik2010-be`, and MP2RAGE {cite:p}`Marques2010-po,Marques2013-yg`.\n",
    "\n",
    "\n",
    "In ongoing efforts to standardize T<sub>1</sub> mapping methods, researchers have been actively developing quantitative MRI phantoms {cite:p}`Keenan2018-px`. The International Society for Magnetic Resonance in Medicine (ISMRM) and the National Institute of Standards and Technology (NIST) collaborated on a standard system phantom {cite:p}`Stupic2021-hu`, which was subsequently commercialized (Premium System Phantom, CaliberMRI, Boulder, Colorado). This phantom has since been used in large multicenter studies, such as Bane et al. {cite:p}`Bane2018-wt` which concluded that acquisition protocols and field strength influence accuracy, repeatability, and interplatform reproducibility. Another NIST-led study {cite:p}`Keenan2021-ly` found no significant T<sub>1</sub> discrepancies among measurements using NIST protocols across 27 MRI systems from three vendors at two clinical field strengths.\n",
    "\n",
    "The 2020 ISMRM reproducibility challenge [^rrsg2020-challenge] posed a slightly different question: can an imaging protocol, independently implemented at multiple centers, consistently measure one of the fundamental MRI parameters (T<sub>1</sub>)? To assess this, we proposed using inversion recovery on a standardized phantom (ISMRM/NIST system phantom) and the healthy human brain. Specifically, **this challenge explored whether the acquisition details provided in a seminal paper on T<sub>1</sub> mapping {cite:p}`Barral2010-qm` is sufficient to ensure the reproducibility across independent research groups.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 &nbsp; &nbsp; | &nbsp; &nbsp; METHODS\n",
    "\n",
    "## 2.1 &nbsp; &nbsp; | &nbsp; &nbsp; Phantom and human data\n",
    "\n",
    "\n",
    "The challenge asked researchers with access to the ISMRM/NIST system phantom {cite:p}`Stupic2021-hu` (Premium System Phantom, CaliberMRI, Boulder, Colorado) to measure T<sub>1</sub> maps of the phantom’s T<sub>1</sub> plate ({numref}`table1`). Researchers who participated in the challenge were instructed to record the temperature before and after scanning the phantom using the phantom's internal thermometer. Instructions for positioning and setting up the phantom were devised by NIST and were provided to researchers through the NIST website [^nist-website]. In brief, the instructions explained how to orient the phantom and how long the phantom should be in the scanner room prior to scanning to achieve thermal equilibrium.\n",
    "\n",
    "```{list-table} Reference [^phantom-reference] T<sub>1</sub> values of the NiCl<sub>2</sub> array of the standard system phantom (for both phantom versions) measured at 20 °C and 3T. Phantoms with serial numbers 0042 or less are referred to as “Version 1”, and those 0043 or greater are “Version 2”.\n",
    ":header-rows: 1\n",
    ":name: table1\n",
    "\n",
    "* - Sphere\n",
    "  - Version 1 (ms)\n",
    "  - Version 2 (ms)\n",
    "* - 1  \n",
    "  - 1989 ± 1.0\n",
    "  - 1883.97 ± 30.32\n",
    "* - 2  \n",
    "  - 1454 ± 2.5\n",
    "  - 1330.16 ± 20.41\n",
    "* - 3  \n",
    "  - 984.1 ± 0.33\n",
    "  - 987.27 ± 14.22\n",
    "* - 4  \n",
    "  - 706 ± 1.0\n",
    "  - 690.08 ± 10.12\n",
    "* - 5  \n",
    "  - 496.7 ± 0.41\n",
    "  - 484.97 ± 7.06\n",
    "* - 6  \n",
    "  - 351.5 ± 0.91\n",
    "  - 341.58 ± 4.97\n",
    "* - 7  \n",
    "  - 247.13 ± 0.086\n",
    "  - 240.86 ± 3.51\n",
    "* - 8  \n",
    "  - 175.3 ± 0.11\n",
    "  - 174.95 ± 2.48\n",
    "* - 9  \n",
    "  - 125.9 ± 0.33\n",
    "  - 121.08 ± 1.75\n",
    "* - 10  \n",
    "  - 89.0 ± 0.17\n",
    "  - 85.75 ± 1.24\n",
    "* - 11  \n",
    "  - 62.7 ± 0.13\n",
    "  - 60.21 ± 0.87\n",
    "* - 12  \n",
    "  - 44.53 ± 0.090\n",
    "  - 42.89 ± 0.44\n",
    "* - 13  \n",
    "  - 30.84 ± 0.016\n",
    "  - 30.40 ± 0.62\n",
    "* - 14  \n",
    "  - 21.719 ± 0.005\n",
    "  - 21.44 ± 0.31\n",
    "```\n",
    "\n",
    "Researchers were also instructed to collect T<sub>1</sub> maps in healthy human brains, and were asked to measure a single slice positioned parallel to the anterior commissure - posterior commissure (AC-PC) line. Prior to imaging, the imaging subjects consented [^informed-consent] to share their de-identified data with the challenge organizers and on the Open Science Framework ([OSF.io](http://www.OSF.io)) website. As the submitted data was a single slice, the researchers were not instructed to de-face the data of their imaging subjects. Researchers submitting human data provided written confirmation to the organizers that their data was acquired in accordance with their institutional ethics committee (or equivalent regulatory body) and that the subjects had consented to data sharing as outlined in the challenge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 &nbsp; &nbsp; | &nbsp; &nbsp; MRI Acquisition Protocol\n",
    "\n",
    "Researchers followed the inversion recovery T<sub>1</sub> mapping protocol optimized for the human brain as described in the paper published by Barral et al. {cite:p}`Barral2010-qm`, which used: TR = 2550 ms, TIs = 50, 400, 1100, 2500 ms, TE = 14 ms, 2 mm slice thickness and 1×1 mm<sup>2</sup> in-plane resolution. Note that this protocol is not suitable for fitting models that assume TR > 5T1. Instead, the more general Barral et al. {cite:p}`Barral2010-qm` fitting model described in Section 2.4 can be used, and this model is compatible with both magnitude-only and complex data. Researchers were instructed to closely adhere to this protocol and report any deviations due to technical limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 &nbsp; &nbsp; | &nbsp; &nbsp; Data Submissions\n",
    "\n",
    "Data submissions for the challenge were handled through a GitHub repository ([https://github.com/rrsg2020/data_submission](https://github.com/rrsg2020/data_submission)), enabling a standardized and transparent process. All datasets were converted to the NIfTI format, and images for all TIs were concatenated into a single NIfTI file. Each submission included a YAML file to store additional information (submitter details, acquisition details, and phantom or human subject details). Submissions were reviewed [^submission-review], and following acceptance the datasets were uploaded to OSF.io ([osf.io/ywc9g/](http://www.osf.io/ywc9g/)). A Jupyter Notebook {cite:p}`Kluyver2016-nl,Beg2021-ps` pipeline using qMRLab {cite:p}`Karakuzu2020-ul,Cabana2015-zg` was used to process the T<sub>1</sub> maps and to conduct quality-control checks. MyBinder links to Jupyter notebooks that reproduced each T<sub>1</sub> map were shared in each respective submission GitHub issue to easily reproduce the results in web browsers while maintaining consistent computational environments. Eighteen submissions were included in the analysis, which resulted in 39 T<sub>1</sub> maps of the NIST/system phantom, and 56 brain T<sub>1</sub> maps. Figure 1 illustrates all the submissions that acquired phantom data (Figure 1-a) and human data (Figure 1-b), the MRI scanner vendors, and the resulting T<sub>1</sub> mapping datasets. Some submissions included measurements where both complex and magnitude-only data from the same acquisition were used to fit T<sub>1</sub> maps, thus the total number of unique acquisitions is lower than the numbers reported above (27 for phantom data and 44 for human data). The datasets were collected on systems from three MRI manufacturers (Siemens, GE, Philips) and were acquired at 3T [^three-t], except for one dataset acquired at 0.35T (the ViewRay MRidian MR-linac).\n",
    "\n",
    "```{figure} images/figure_1.png\n",
    "---\n",
    "width: 900px\n",
    "name: fig1\n",
    "align: center\n",
    "---\n",
    "```\n",
    "\n",
    "<p class=\"caption\">\n",
    "<b>Figure 1</b>. List of the datasets submitted to the challenge. Submissions that included phantom data are shown in a), and those that included human brain data are shown in b). For the phantom (panel a), each submission acquired its data using a single phantom, but some researchers shared the same physical phantom with each other. Green indicates submissions used for inter-submission analyses, and orange indicates the sites used for intra-submission analyses. T<sub>1</sub> maps used in the calculations of inter- (green) and intra- (orange) submission coefficients of variation (COV) are indicated with asterisks. Images c) and d) illustrate the ROI choice in phantoms and humans.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 &nbsp; &nbsp; | &nbsp; &nbsp; Fitting Model and Pipeline\n",
    "\n",
    "A reduced-dimension non-linear least squares (RD-NLS) approach was used to fit the complex general inversion recovery signal equation:\n",
    "\n",
    "```{math}\n",
    ":label: my_label\n",
    "S(TI) = a + be^{-TI/T_1}\n",
    "```\n",
    "\n",
    "where a and b are complex constants. This approach, developed by Barral et al. {cite:p}`Barral2010-qm`, offers a model for the general T<sub>1</sub> signal equation without relying on the long-TR approximation. The a and b constants inherently factor TR in them, as well as other imaging parameters such as excitation pulse angle, inversion pulse flip angles, TR, TE, TI, and a constant that has contributions from T<sub>2</sub> and the receive coil sensitivity. Barral et al. [31] shared their MATLAB (MathWorks, Natick, MA) code for the fitting algorithm used in their paper [^their-paper]. Magnitude-only data were fitted to a modified version of Eq. 1 (Eq. 15 of Barral et al. 2010) with signal-polarity restoration by finding the signal minima, fitting the inversion recovery curve for two cases (data points for TI < TI<sub>minimum</sub> flipped, and data points for TI ≤ TI<sub>minimum</sub> flipped), and selecting the case that resulted in the best fit based on minimizing the residual between the model and the measurements [^residual]. This code is available as part of the open-source software qMRLab {cite:p}`Karakuzu2020-ul,Cabana2015-zg`, which provides a standardized application program interface (API) to call the fitting in MATLAB/Octave scripts.\n",
    "\n",
    "\n",
    "A data processing pipeline was written using MATLAB/Octave in a Jupyter Notebook. This pipeline downloads every dataset from OSF.io ([osf.io/ywc9g/](http://www.osf.io/ywc9g/)), loads its configuration file, fits the T</sub>1</sub> maps, and then saves them to NIfTI and PNG formats. The code is available on GitHub ([https://github.com/rrsg2020/t1_fitting_pipeline](https://github.com/rrsg2020/t1_fitting_pipeline), filename: RRSG_T1_fitting.ipynb). Finally, T<sub>1</sub> maps were manually uploaded to OSF ([osf.io/ywc9g/](http://WWW.osf.io/ywc9g/)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 &nbsp; &nbsp; | &nbsp; &nbsp; Image Labeling & Registration\n",
    "\n",
    "The T<sub>1</sub> plate (NiCl<sub>2</sub> array) of the phantom has 14 spheres that were labeled as the regions-of-interest (ROI) using a numerical mask template created in MATLAB, provided by NIST researchers (Figure 1-c). To avoid potential edge effects in the T<sub>1</sub> maps, the ROI labels were reduced to 60% of the expected sphere diameter. A registration pipeline in Python using the Advanced Normalization Tools (ANTs) {cite}`Avants2009-cw` was developed and shared in the analysis repository of our GitHub organization ([https://github.com/rrsg2020/analysis](https://github.com/rrsg2020/analysis), filename: register_t1maps_nist.py, commit ID: 8d38644). Briefly, a label-based registration was first applied to obtain a coarse alignment, followed by an affine registration (gradientStep: 0.1, metric: cross correlation, number of steps: 3, iterations: 100/100/100, smoothness: 0/0/0, sub-sampling: 4/2/1) and a BSplineSyN registration (gradientStep:0.5, meshSizeAtBaseLevel:3, number of steps: 3, iterations: 50/50/10, smoothness: 0/0/0, sub-sampling: 4/2/1). The ROI labels template was nonlinearly registered to each T<sub>1</sub> map uploaded to OSF.\n",
    "\n",
    "\n",
    "For human data, manual ROIs were segmented by a single researcher (M.B., 11+ years of neuroimaging experience) using FSLeyes {cite}`McCarthy2019-qd` in four regions (Figure 1-d): located in the genu, splenium, deep gray matter, and cortical gray matter. Automatic segmentation was not used because the data were single-slice and there was inconsistent slice positioning between datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 &nbsp; &nbsp; | &nbsp; &nbsp; Analysis and Statistics\n",
    "\n",
    "Analysis code and scripts were developed and shared in a version-controlled public GitHub repository [^public-repo]. The T<sub>1</sub> fitting and data analysis were performed by M.B., one of the challenge organizers. Computational environment requirements were containerized in Docker {cite:p}`Merkel2014-cu,Boettiger2015-vd` to create an executable environment that allows for analysis reproduction in a web browser via MyBinder [^my-binder] {cite:p}`Project_Jupyter2018-ll`. Backend Python files handled reference data, database operations, ROI masking, and general analysis tools. Configuration files handled dataset information, and the datasets were downloaded and pooled using a script (`make_pooled_datasets.py`). The databases were created using a reproducible Jupyter Notebook script and subsequently saved in the repository.\n",
    "\n",
    "\n",
    "The mean T<sub>1</sub> values of the ISMRM/NIST phantom data for each ROI were compared with temperature-corrected reference values and visualized in three different types of plots (linear axes, log-log axes, and error relative to the reference value). Temperature correction involved nonlinear interpolation [^nonlinear] of a NIST reference table of T<sub>1</sub> values for temperatures ranging from 16 °C to 26 °C (2 °C intervals) as specified in the phantom’s technical specifications. For the human datasets, the mean and standard deviations for each tissue ROI were calculated from all submissions across all sites. Two submissions (one of phantom data – submission 6 in Figure 1-a, and one of human data – submission 18 in Figure 1-b) were received that measured large T<sub>1</sub> mapping datasets. Submission 6 consisted of data from one traveling phantom acquired at seven Philips imaging sites, and submission 18 was a large cohort of volunteers that were imaged on two 3T scanners, one GE and one Philips. These datasets (identified in orange in Figures 1, 3, and 4) were used to calculate intra-submission coefficients of variation (COV) (one per scanner/volunteer, identified by asterisks in Figure 1-a and 1-b), and inter-submission COVs were calculated using one T<sub>1</sub> map from each of these (orange) along with one from all other submissions [^phantom-version] (identified as green in Figures 1, 3, and 4, and the T<sub>1</sub> maps used in those COV calculations are also indicated with asterisks in Figure 1-a and 1-b). All quality assurance and analysis plot images were stored in the repository. Additionally, the database files of ROI values and acquisition details for all submissions were also stored in the repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 &nbsp; &nbsp; | &nbsp; &nbsp; Dashboard\n",
    "\n",
    "To widely disseminate the challenge results, a web-based dashboard was developed (Figure 2, <a href=\"https://rrsg2020.dashboards.neurolibre.org\">https://rrsg2020.dashboards.neurolibre.org</a>). The landing page (Figure 2-a) showcases the relationship between the phantom and brain datasets acquired at different sites/vendors. Selecting the Phantom or In Vivo icons and then clicking a ROI will display whisker plots for that region. Additional sections of the dashboard allow for displaying statistical summaries for both sets of data, a magnitude vs complex data fitting comparison, and hierarchical shift function analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe src=\"https://rrsg2020.dashboards.neurolibre.org\" width=\"120%\" height=\"750px\" style=\"border:none;margin: 0 -10%\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"caption\">\n",
    "<b>Figure 2</b> Dashboard. a) Welcome page listing all the sites, the types of subject, and scanner vendor, and the relationship between the three. b) The phantom tab for a selected ROI, and c) The in vivo tab for a selected ROI. Link: [https://rrsg2020.dashboards.neurolibre.org](https://rrsg2020.dashboards.neurolibre.org)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 &nbsp; &nbsp; | &nbsp; &nbsp; RESULTS\n",
    "\n",
    "Figure 3 presents a comprehensive overview of the challenge results through violin plots, depicting inter- and intra- submission comparisons in both phantoms (a) and human (b) datasets. For the phantom (Figure 3-a), the average inter-submission COV for the first five spheres, representing the expected T<sub>1</sub> value range in the human brain (approximately 500 to 2000 ms) was 6.1%. By addressing outliers from two sites associated with specific challenges for sphere 4 (signal null near a TI), the mean inter-submission COV was reduced to 4.1%. One participant (submission 6, Figure 1) measured T<sub>1</sub> maps using a consistent protocol at 7 different sites, and the mean intra-submission COV across the first five spheres for this submission was calculated to be 2.9%.\n",
    "\n",
    "\n",
    "For the human datasets (Figure 3-b), inter-submission COVs for independently-implemented imaging protocols were 5.9% for genu, 10.6 % for splenium, 16 % for cortical GM, and 22% for deep GM. One participant (submission 18, Figure 1) measured a large dataset (13 individuals) on three scanners and two vendors, and the intra-submission COVs for this submission were 3.2% for genu, 3.1% for splenium, 6.9% for cortical GM, and 7.1% for deep GM. The binomial appearance for the splenium, deep GM, and cortical GM for the sites used in the inter-site analyses (green) can be explained by an outlier measurement, which can be seen in (Figure 4 e-f, site 3.001).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_output",
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if build == 'latest':\n",
    "    if path.isdir('analysis')== False:\n",
    "        !git clone https://github.com/rrsg2020/analysis.git\n",
    "        dir_name = 'analysis'\n",
    "        analysis = os.listdir(dir_name)\n",
    "\n",
    "        for item in analysis:\n",
    "            if item.endswith(\".ipynb\"):\n",
    "                os.remove(os.path.join(dir_name, item))\n",
    "            if item.endswith(\".md\"):\n",
    "                os.remove(os.path.join(dir_name, item))\n",
    "elif build == 'archive':\n",
    "    if os.path.isdir(Path('../data')):\n",
    "        data_path = ['../data/rrsg-2020-note/rrsg-2020-neurolibre']\n",
    "    else:\n",
    "        # define data requirement path\n",
    "        data_req_path = os.path.join(\"..\", \"binder\", \"data_requirement.json\")\n",
    "        # download data\n",
    "        repo2data = Repo2Data(data_req_path)\n",
    "        data_path = repo2data.install()\n",
    "\n",
    "from analysis.src.database import *\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('analysis/custom_matplotlibrc')\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "fig_id = 0\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 1)\n",
    "\n",
    "\n",
    "\n",
    "# Configuration\n",
    "database_path = Path('analysis/databases/3T_NIST_T1maps_database.pkl')\n",
    "remove_outliers = False\n",
    "\n",
    "# Load database\n",
    "df = pd.read_pickle(database_path)\n",
    "\n",
    "# Prepare stats table\n",
    "\n",
    "columns = [\n",
    "    '1',\n",
    "    '2',\n",
    "    '3',\n",
    "    '4',\n",
    "    '5',\n",
    "    '6',\n",
    "    '7',\n",
    "    '8',\n",
    "    '9',\n",
    "    '10',\n",
    "    '11',\n",
    "    '12',\n",
    "    '13',\n",
    "    '14'\n",
    "]\n",
    "\n",
    "col_vals = [\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None\n",
    "]\n",
    "\n",
    "df_setup = {\n",
    "    'all submissions mean T1': col_vals,\n",
    "    'inter-submission mean T1': col_vals,\n",
    "    'intra-submission mean T1': col_vals,\n",
    "    'all submissions T1 STD': col_vals,\n",
    "    'inter-submission T1 STD': col_vals,\n",
    "    'intra-submission T1 STD': col_vals,\n",
    "    'all submissions COV [%]': col_vals,\n",
    "    'inter-submission COV [%]': col_vals,\n",
    "    'intra-submission COV [%]': col_vals\n",
    "}\n",
    "\n",
    "stats_table = pd.DataFrame.from_dict(df_setup, orient='index', columns=columns)\n",
    "\n",
    "## Inter-submission stats (phantom version 1)\n",
    "# One subject/measurement per submission were selected. The protocol closest to the proposed one was selected, preferring complex data over magnitude-only.\n",
    "\n",
    "# Selecting one subject/measurement per submission, protocol closest to proposed, complex if available.\n",
    "ids_intersubmissions = (\n",
    "    2.001,\n",
    "    3.001,\n",
    "    5.002,\n",
    "    6.002,\n",
    "    8.001,\n",
    "    12.001,\n",
    ")\n",
    "\n",
    "\n",
    "estimate_1 = np.array([])\n",
    "estimate_2 = np.array([])\n",
    "estimate_3 = np.array([])\n",
    "estimate_4 = np.array([])\n",
    "estimate_5 = np.array([])\n",
    "estimate_6 = np.array([])\n",
    "estimate_7 = np.array([])\n",
    "estimate_8 = np.array([])\n",
    "estimate_9 = np.array([])\n",
    "estimate_10 = np.array([])\n",
    "estimate_11 = np.array([])\n",
    "estimate_12 = np.array([])\n",
    "estimate_13 = np.array([])\n",
    "estimate_14 = np.array([])\n",
    "print(estimate_1.size)\n",
    "ii = 0\n",
    "for index in ids_intersubmissions:\n",
    "    if index == 13.001: # missing data\n",
    "        break\n",
    "    if remove_outliers and (index == 12.001 or index == 2.001):\n",
    "            pass\n",
    "    else:\n",
    "        if df.loc[index]['phantom serial number']<42: # version 1\n",
    "            pass\n",
    "        else:\n",
    "            if df.loc[index]['phantom serial number'] is None:\n",
    "                df.at[index, 'phantom serial number'] = 999  # Missing phantom number was version 2, see discussion here https://github.com/rrsg2020/data_submission/issues/25#issuecomment-620045155\n",
    "            estimate_1 = np.append(estimate_1, np.mean(df.loc[index]['T1 - NIST sphere 1']))\n",
    "            estimate_2 = np.append(estimate_2, np.mean(df.loc[index]['T1 - NIST sphere 2']))\n",
    "            estimate_3 = np.append(estimate_3, np.mean(df.loc[index]['T1 - NIST sphere 3']))\n",
    "            estimate_4 = np.append(estimate_4, np.mean(df.loc[index]['T1 - NIST sphere 4']))\n",
    "            estimate_5 = np.append(estimate_5, np.mean(df.loc[index]['T1 - NIST sphere 5']))\n",
    "            estimate_6 = np.append(estimate_6, np.mean(df.loc[index]['T1 - NIST sphere 6']))\n",
    "            estimate_7 = np.append(estimate_7, np.mean(df.loc[index]['T1 - NIST sphere 7']))\n",
    "            estimate_8 = np.append(estimate_8, np.mean(df.loc[index]['T1 - NIST sphere 8']))\n",
    "            estimate_9 = np.append(estimate_9, np.mean(df.loc[index]['T1 - NIST sphere 9']))\n",
    "            estimate_10 = np.append(estimate_10, np.mean(df.loc[index]['T1 - NIST sphere 10']))\n",
    "            estimate_11 = np.append(estimate_11, np.mean(df.loc[index]['T1 - NIST sphere 11']))\n",
    "            estimate_12 = np.append(estimate_12, np.mean(df.loc[index]['T1 - NIST sphere 12']))\n",
    "            estimate_13 = np.append(estimate_13, np.mean(df.loc[index]['T1 - NIST sphere 13']))\n",
    "            estimate_14 = np.append(estimate_14, np.mean(df.loc[index]['T1 - NIST sphere 14']))\n",
    "    \n",
    "        ii = ii +1\n",
    "\n",
    "stats_table['1']['inter-submission mean T1'] = np.nanmean(estimate_1)\n",
    "stats_table['1']['inter-submission T1 STD'] = np.nanstd(estimate_1)\n",
    "stats_table['1']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_1),np.nanmean(estimate_1)) * 100\n",
    "\n",
    "stats_table['2']['inter-submission mean T1'] = np.nanmean(estimate_2)\n",
    "stats_table['2']['inter-submission T1 STD'] = np.nanstd(estimate_2)\n",
    "stats_table['2']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_2),np.nanmean(estimate_2)) * 100\n",
    "\n",
    "stats_table['3']['inter-submission mean T1'] = np.nanmean(estimate_3)\n",
    "stats_table['3']['inter-submission T1 STD'] = np.nanstd(estimate_3)\n",
    "stats_table['3']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_3),np.nanmean(estimate_3)) * 100\n",
    "\n",
    "stats_table['4']['inter-submission mean T1'] = np.nanmean(estimate_4)\n",
    "stats_table['4']['inter-submission T1 STD'] = np.nanstd(estimate_4)\n",
    "stats_table['4']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_4),np.nanmean(estimate_4)) * 100\n",
    "\n",
    "stats_table['5']['inter-submission mean T1'] = np.nanmean(estimate_5)\n",
    "stats_table['5']['inter-submission T1 STD'] = np.nanstd(estimate_5)\n",
    "stats_table['5']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_5),np.nanmean(estimate_5)) * 100\n",
    "\n",
    "stats_table['6']['inter-submission mean T1'] = np.nanmean(estimate_6)\n",
    "stats_table['6']['inter-submission T1 STD'] = np.nanstd(estimate_6)\n",
    "stats_table['6']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_6),np.nanmean(estimate_6)) * 100\n",
    "\n",
    "stats_table['7']['inter-submission mean T1'] = np.nanmean(estimate_7)\n",
    "stats_table['7']['inter-submission T1 STD'] = np.nanstd(estimate_7)\n",
    "stats_table['7']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_7),np.nanmean(estimate_7)) * 100\n",
    "\n",
    "stats_table['8']['inter-submission mean T1'] = np.nanmean(estimate_8)\n",
    "stats_table['8']['inter-submission T1 STD'] = np.nanstd(estimate_8)\n",
    "stats_table['8']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_8),np.nanmean(estimate_8)) * 100\n",
    "\n",
    "stats_table['9']['inter-submission mean T1'] = np.nanmean(estimate_9)\n",
    "stats_table['9']['inter-submission T1 STD'] = np.nanstd(estimate_9)\n",
    "stats_table['9']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_9),np.nanmean(estimate_9)) * 100\n",
    "\n",
    "stats_table['10']['inter-submission mean T1'] = np.nanmean(estimate_10)\n",
    "stats_table['10']['inter-submission T1 STD'] = np.nanstd(estimate_10)\n",
    "stats_table['10']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_10),np.nanmean(estimate_10)) * 100\n",
    "\n",
    "stats_table['11']['inter-submission mean T1'] = np.nanmean(estimate_11)\n",
    "stats_table['11']['inter-submission T1 STD'] = np.nanstd(estimate_11)\n",
    "stats_table['11']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_11),np.nanmean(estimate_11)) * 100\n",
    "\n",
    "stats_table['12']['inter-submission mean T1'] = np.nanmean(estimate_12)\n",
    "stats_table['12']['inter-submission T1 STD'] = np.nanstd(estimate_12)\n",
    "stats_table['12']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_12),np.nanmean(estimate_12)) * 100\n",
    "\n",
    "stats_table['13']['inter-submission mean T1'] = np.nanmean(estimate_13)\n",
    "stats_table['13']['inter-submission T1 STD'] = np.nanstd(estimate_13)\n",
    "stats_table['13']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_13),np.nanmean(estimate_13)) * 100\n",
    "\n",
    "stats_table['14']['inter-submission mean T1'] = np.nanmean(estimate_14)\n",
    "stats_table['14']['inter-submission T1 STD'] = np.nanstd(estimate_14)\n",
    "stats_table['14']['inter-submission COV [%]'] = np.divide(np.nanstd(estimate_14),np.nanmean(estimate_14)) * 100\n",
    "\n",
    "estimate_1_inter = estimate_1\n",
    "estimate_2_inter = estimate_2\n",
    "estimate_3_inter = estimate_3\n",
    "estimate_4_inter = estimate_4\n",
    "estimate_5_inter = estimate_5\n",
    "\n",
    "## Intra-submission stats\n",
    "# All unique T1 maps from Site 6 were used for the stats calculations. One single vendor, 7 sites. Complex data selected.\n",
    "\n",
    "# Selecting one submission\n",
    "ids_intrasubmissions = (\n",
    "6.002,\n",
    "6.004,\n",
    "6.006,\n",
    "6.008,\n",
    "6.010,\n",
    "6.012,\n",
    "6.014,\n",
    ")\n",
    "\n",
    "\n",
    "estimate_1 = np.array([])\n",
    "estimate_2 = np.array([])\n",
    "estimate_3 = np.array([])\n",
    "estimate_4 = np.array([])\n",
    "estimate_5 = np.array([])\n",
    "estimate_6 = np.array([])\n",
    "estimate_7 = np.array([])\n",
    "estimate_8 = np.array([])\n",
    "estimate_9 = np.array([])\n",
    "estimate_10 = np.array([])\n",
    "estimate_11 = np.array([])\n",
    "estimate_12 = np.array([])\n",
    "estimate_13 = np.array([])\n",
    "estimate_14 = np.array([])\n",
    "\n",
    "ii = 0\n",
    "for index in ids_intrasubmissions:\n",
    "    \n",
    "    if index == 13.001: # missing data\n",
    "        break\n",
    "    if remove_outliers and (index == 12.001 or index == 2.001):\n",
    "            pass\n",
    "    else:\n",
    "        estimate_1 = np.append(estimate_1, np.mean(df.loc[index]['T1 - NIST sphere 1']))\n",
    "        estimate_2 = np.append(estimate_2, np.mean(df.loc[index]['T1 - NIST sphere 2']))\n",
    "        estimate_3 = np.append(estimate_3, np.mean(df.loc[index]['T1 - NIST sphere 3']))\n",
    "        estimate_4 = np.append(estimate_4, np.mean(df.loc[index]['T1 - NIST sphere 4']))\n",
    "        estimate_5 = np.append(estimate_5, np.mean(df.loc[index]['T1 - NIST sphere 5']))\n",
    "        estimate_6 = np.append(estimate_6, np.mean(df.loc[index]['T1 - NIST sphere 6']))\n",
    "        estimate_7 = np.append(estimate_7, np.mean(df.loc[index]['T1 - NIST sphere 7']))\n",
    "        estimate_8 = np.append(estimate_8, np.mean(df.loc[index]['T1 - NIST sphere 8']))\n",
    "        estimate_9 = np.append(estimate_9, np.mean(df.loc[index]['T1 - NIST sphere 9']))\n",
    "        estimate_10 = np.append(estimate_10, np.mean(df.loc[index]['T1 - NIST sphere 10']))\n",
    "        estimate_11 = np.append(estimate_11, np.mean(df.loc[index]['T1 - NIST sphere 11']))\n",
    "        estimate_12 = np.append(estimate_12, np.mean(df.loc[index]['T1 - NIST sphere 12']))\n",
    "        estimate_13 = np.append(estimate_13, np.mean(df.loc[index]['T1 - NIST sphere 13']))\n",
    "        estimate_14 = np.append(estimate_14, np.mean(df.loc[index]['T1 - NIST sphere 14']))\n",
    "\n",
    "        ii = ii +1\n",
    "\n",
    "stats_table['1']['intra-submission mean T1'] = np.nanmean(estimate_1)\n",
    "stats_table['1']['intra-submission T1 STD'] = np.nanstd(estimate_1)\n",
    "stats_table['1']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_1),np.nanmean(estimate_1)) * 100\n",
    "\n",
    "stats_table['2']['intra-submission mean T1'] = np.nanmean(estimate_2)\n",
    "stats_table['2']['intra-submission T1 STD'] = np.nanstd(estimate_2)\n",
    "stats_table['2']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_2),np.nanmean(estimate_2)) * 100\n",
    "\n",
    "stats_table['3']['intra-submission mean T1'] = np.nanmean(estimate_3)\n",
    "stats_table['3']['intra-submission T1 STD'] = np.nanstd(estimate_3)\n",
    "stats_table['3']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_3),np.nanmean(estimate_3)) * 100\n",
    "\n",
    "stats_table['4']['intra-submission mean T1'] = np.nanmean(estimate_4)\n",
    "stats_table['4']['intra-submission T1 STD'] = np.nanstd(estimate_4)\n",
    "stats_table['4']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_4),np.nanmean(estimate_4)) * 100\n",
    "\n",
    "stats_table['5']['intra-submission mean T1'] = np.nanmean(estimate_5)\n",
    "stats_table['5']['intra-submission T1 STD'] = np.nanstd(estimate_5)\n",
    "stats_table['5']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_5),np.nanmean(estimate_5)) * 100\n",
    "\n",
    "stats_table['6']['intra-submission mean T1'] = np.nanmean(estimate_6)\n",
    "stats_table['6']['intra-submission T1 STD'] = np.nanstd(estimate_6)\n",
    "stats_table['6']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_6),np.nanmean(estimate_6)) * 100\n",
    "\n",
    "stats_table['7']['intra-submission mean T1'] = np.nanmean(estimate_7)\n",
    "stats_table['7']['intra-submission T1 STD'] = np.nanstd(estimate_7)\n",
    "stats_table['7']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_7),np.nanmean(estimate_7)) * 100\n",
    "\n",
    "stats_table['8']['intra-submission mean T1'] = np.nanmean(estimate_8)\n",
    "stats_table['8']['intra-submission T1 STD'] = np.nanstd(estimate_8)\n",
    "stats_table['8']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_8),np.nanmean(estimate_8)) * 100\n",
    "\n",
    "stats_table['9']['intra-submission mean T1'] = np.nanmean(estimate_9)\n",
    "stats_table['9']['intra-submission T1 STD'] = np.nanstd(estimate_9)\n",
    "stats_table['9']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_9),np.nanmean(estimate_9)) * 100\n",
    "\n",
    "stats_table['10']['intra-submission mean T1'] = np.nanmean(estimate_10)\n",
    "stats_table['10']['intra-submission T1 STD'] = np.nanstd(estimate_10)\n",
    "stats_table['10']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_10),np.nanmean(estimate_10)) * 100\n",
    "\n",
    "stats_table['11']['intra-submission mean T1'] = np.nanmean(estimate_11)\n",
    "stats_table['11']['intra-submission T1 STD'] = np.nanstd(estimate_11)\n",
    "stats_table['11']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_11),np.nanmean(estimate_11)) * 100\n",
    "\n",
    "stats_table['12']['intra-submission mean T1'] = np.nanmean(estimate_12)\n",
    "stats_table['12']['intra-submission T1 STD'] = np.nanstd(estimate_12)\n",
    "stats_table['12']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_12),np.nanmean(estimate_12)) * 100\n",
    "\n",
    "stats_table['13']['intra-submission mean T1'] = np.nanmean(estimate_13)\n",
    "stats_table['13']['intra-submission T1 STD'] = np.nanstd(estimate_13)\n",
    "stats_table['13']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_13),np.nanmean(estimate_13)) * 100\n",
    "\n",
    "stats_table['14']['intra-submission mean T1'] = np.nanmean(estimate_14)\n",
    "stats_table['14']['intra-submission T1 STD'] = np.nanstd(estimate_14)\n",
    "stats_table['14']['intra-submission COV [%]'] = np.divide(np.nanstd(estimate_14),np.nanmean(estimate_14)) * 100\n",
    "\n",
    "estimate_1_intra = estimate_1\n",
    "estimate_2_intra = estimate_2\n",
    "estimate_3_intra = estimate_3\n",
    "estimate_4_intra = estimate_4\n",
    "estimate_5_intra = estimate_5\n",
    "\n",
    "## Violin plot\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Identify common lengths for each group\n",
    "lengths = [len(estimate_1_inter), len(estimate_1_intra),\n",
    "           len(estimate_2_inter), len(estimate_2_intra),\n",
    "           len(estimate_3_inter), len(estimate_3_intra),\n",
    "           len(estimate_4_inter), len(estimate_4_intra),\n",
    "           len(estimate_5_inter), len(estimate_5_intra)]\n",
    "\n",
    "# Find the minimum length\n",
    "min_length = min(lengths)\n",
    "\n",
    "# Slice arrays to the minimum length\n",
    "estimate_1_inter = estimate_1_inter[:min_length]\n",
    "estimate_1_intra = estimate_1_intra[:min_length]\n",
    "\n",
    "estimate_2_inter = estimate_2_inter[:min_length]\n",
    "estimate_2_intra = estimate_2_intra[:min_length]\n",
    "\n",
    "estimate_3_inter = estimate_3_inter[:min_length]\n",
    "estimate_3_intra = estimate_3_intra[:min_length]\n",
    "\n",
    "estimate_4_inter = estimate_4_inter[:min_length]\n",
    "estimate_4_intra = estimate_4_intra[:min_length]\n",
    "\n",
    "estimate_5_inter = estimate_5_inter[:min_length]\n",
    "estimate_5_intra = estimate_5_intra[:min_length]\n",
    "\n",
    "# Combine data into a single DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'T1 (ms)': np.concatenate([\n",
    "        estimate_1_inter, estimate_1_intra,\n",
    "        estimate_2_inter, estimate_2_intra,\n",
    "        estimate_3_inter, estimate_3_intra,\n",
    "        estimate_4_inter, estimate_4_intra,\n",
    "        estimate_5_inter, estimate_5_intra\n",
    "    ]),\n",
    "    'Group': ['Sphere 1'] * min_length * 2 +\n",
    "             ['Sphere 2'] * min_length * 2 +\n",
    "             ['Sphere 3'] * min_length * 2 +\n",
    "             ['Sphere 4'] * min_length * 2 +\n",
    "             ['Sphere 5'] * min_length * 2,\n",
    "    'Type': ['Inter'] * min_length + ['Intra'] * min_length +\n",
    "            ['Inter'] * min_length + ['Intra'] * min_length +\n",
    "            ['Inter'] * min_length + ['Intra'] * min_length +\n",
    "            ['Inter'] * min_length + ['Intra'] * min_length +\n",
    "            ['Inter'] * min_length + ['Intra'] * min_length,\n",
    "    '': ['Sphere 1'] * min_length * 2 +\n",
    "                ['Sphere 2'] * min_length * 2 +\n",
    "                ['Sphere 3'] * min_length * 2 +\n",
    "                ['Sphere 4'] * min_length * 2 +\n",
    "                ['Sphere 5'] * min_length * 2,\n",
    "})\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from plotly import __version__\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "config={\n",
    "    'showLink': False,\n",
    "    'displayModeBar': False,\n",
    "    'toImageButtonOptions': {\n",
    "                'format': 'png', # one of png, svg, jpeg, webp\n",
    "                'filename': 'custom_image',\n",
    "                'height': 400,\n",
    "                'width': 800,\n",
    "                'scale': 2 # Multiply title/legend/axis/canvas sizes by this factor\n",
    "            }\n",
    "    }\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Set the color palette\n",
    "inter_color='rgb(113, 180, 159)'\n",
    "intra_color='rgb(233, 150, 117)'\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "df1 = pd.DataFrame({\n",
    "    'T1 (ms)': np.concatenate([estimate_1_inter, estimate_2_inter,  estimate_3_inter, estimate_4_inter, estimate_5_inter]),\n",
    "    'Group': ['<b>Sphere 1</b>'] * len(estimate_1_inter) + ['<b>Sphere 2</b>'] * len(estimate_2_inter) + ['<b>Sphere 3</b>'] * len(estimate_3_inter) + ['<b>Sphere 4</b>'] * len(estimate_4_inter) + ['<b>Sphere 5</b>'] * len(estimate_5_inter), \n",
    "    'Type': ['Inter'] * len(estimate_1_inter)  + ['Inter'] * len(estimate_2_inter)  + ['Inter'] * len(estimate_3_inter) + ['Inter'] * len(estimate_4_inter) + ['Inter'] * len(estimate_5_inter) ,\n",
    "    '':  ['Sphere 1'] * len(estimate_1_inter) + ['Sphere 2'] * len(estimate_2_inter) + ['Sphere 3'] * len(estimate_3_inter) + ['Sphere 4'] * len(estimate_4_inter) + ['Sphere 5'] * len(estimate_5_inter), \n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'T1 (ms)': np.concatenate([estimate_1_intra, estimate_2_intra,  estimate_3_intra, estimate_4_intra, estimate_5_intra]),\n",
    "    'Group': ['<b>Sphere 1</b>'] * len(estimate_1_intra) + ['<b>Sphere 2</b>'] * len(estimate_2_intra) + ['<b>Sphere 3</b>'] * len(estimate_3_intra) + ['<b>Sphere 4</b>'] * len(estimate_4_intra) + ['<b>Sphere 5</b>'] * len(estimate_5_intra), \n",
    "    'Type': ['Inter'] * len(estimate_1_intra)  + ['Inter'] * len(estimate_2_intra)  + ['Inter'] * len(estimate_3_intra) + ['Inter'] * len(estimate_4_intra) + ['Inter'] * len(estimate_5_intra) ,\n",
    "    '':  ['Sphere 1'] * len(estimate_1_intra) + ['Sphere 2'] * len(estimate_2_intra) + ['Sphere 3'] * len(estimate_3_intra) + ['Sphere 4'] * len(estimate_4_intra) + ['Sphere 5'] * len(estimate_5_intra), \n",
    "})\n",
    "\n",
    "fig.add_trace(go.Violin(x=df1['Group'],\n",
    "                        y=df1['T1 (ms)'],\n",
    "                        side='negative',\n",
    "                        line_color='black',\n",
    "                        fillcolor=inter_color, \n",
    "                        line=dict(width=1.5),\n",
    "                        points=False,\n",
    "                        width=1.15,\n",
    "                        showlegend=False)\n",
    "             )\n",
    "fig.add_trace(go.Violin(x=df2['Group'],\n",
    "                        y=df2['T1 (ms)'],\n",
    "                        side='positive',\n",
    "                        line_color='black',\n",
    "                        fillcolor=intra_color, \n",
    "                        line=dict(width=1.5),\n",
    "                        points=False,\n",
    "                        width=1.15,\n",
    "                        showlegend=False)\n",
    "             )\n",
    "\n",
    "fig.update_layout(violingap=0, violinmode='overlay')\n",
    "#fig.show()\n",
    "\n",
    "fig.update_xaxes(\n",
    "    type=\"category\",\n",
    "    autorange=False,\n",
    "    range=[-0.3,4.7],\n",
    "    dtick=1,\n",
    "    showgrid=False,\n",
    "    gridwidth =1,\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    tickfont=dict(\n",
    "        family='Times New Roman',\n",
    "        size=22,\n",
    "    ),\n",
    "    )\n",
    "fig.update_yaxes(\n",
    "    title='<b>T<sub>1</sub> (ms)</b>',\n",
    "    autorange=False,\n",
    "    range=[300, 2300],\n",
    "    showgrid=True,\n",
    "    dtick=250,\n",
    "    gridcolor='rgb(169,169,169)',\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    tickfont=dict(\n",
    "        family='Times New Roman',\n",
    "        size=22,\n",
    "    ),\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    margin=go.layout.Margin(\n",
    "        l=80,\n",
    "        r=40,\n",
    "        b=80,\n",
    "        t=10,\n",
    "    ),\n",
    "    font=dict(\n",
    "        family='Times New Roman',\n",
    "        size=22\n",
    "    ),\n",
    "   \n",
    "    paper_bgcolor='rgb(255, 255, 255)',\n",
    "    plot_bgcolor='rgb(255, 255, 255)',\n",
    "    annotations=[\n",
    "        dict(\n",
    "            x=-0.1,\n",
    "            y=-0.22,\n",
    "            showarrow=False,\n",
    "            text='<b>a</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=64\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "        ),\n",
    "        dict(\n",
    "            x=1,\n",
    "            y=0.96,\n",
    "            showarrow=False,\n",
    "            text='<b>Inter-sites</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=24,\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper',\n",
    "            bgcolor='rgba(102, 194, 165, 0.8)'\n",
    "        ),\n",
    "        dict(\n",
    "            x=1,\n",
    "            y=0.85,\n",
    "            showarrow=False,\n",
    "            text='<b>Intra-sites</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=24,\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper',\n",
    "            bgcolor='rgba(252, 141, 98, 0.8)'\n",
    "        ),   \n",
    "    ]\n",
    ")\n",
    "\n",
    "# update characteristics shared by all traces\n",
    "fig.update_traces(quartilemethod='linear',\n",
    "                  scalemode='count',\n",
    "                  meanline=dict(visible=False),\n",
    "                  )\n",
    "\n",
    "#iplot(fig, filename = 'figure3', config = config)\n",
    "plot(fig, filename = 'figure3a.html', config = config)\n",
    "\n",
    "# Configurations\n",
    "database_path = Path('analysis/databases/3T_human_T1maps_database.pkl')\n",
    "\n",
    "#Load database\n",
    "df = pd.read_pickle(database_path)\n",
    "\n",
    "# Prepare stats table\n",
    "columns = [\n",
    "    'genu WM',\n",
    "    'splenium WM',\n",
    "    'cortical GM',\n",
    "    'deep GM'\n",
    "]\n",
    "\n",
    "col_vals = [\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None\n",
    "]\n",
    "\n",
    "df_setup = {\n",
    "    'all submissions mean T1': col_vals,\n",
    "    'inter-submission mean T1': col_vals,\n",
    "    'intra-submission mean T1': col_vals,\n",
    "    'all submissions T1 STD': col_vals,\n",
    "    'inter-submission T1 STD': col_vals,\n",
    "    'intra-submission T1 STD': col_vals,\n",
    "    'all submissions COV [%]': col_vals,\n",
    "    'inter-submission COV [%]': col_vals,\n",
    "    'intra-submission COV [%]': col_vals\n",
    "}\n",
    "\n",
    "stats_table = pd.DataFrame.from_dict(df_setup, orient='index', columns=columns)\n",
    "\n",
    "## Inter-submission stats\n",
    "# One subject/measurement per submission were selected. The protocol closest to the proposed one was selected, preferring complex data over magnitude-only.\n",
    "\n",
    "# Selecting one subject/measurement per submission, protocol closest to proposed, complex if available.\n",
    "ids_intersubmissions = (\n",
    "    1.001,\n",
    "    2.003,\n",
    "    3.001,\n",
    "    5.001,\n",
    "    6.001,\n",
    "    7.001,\n",
    "    8.002,\n",
    "    9.001,\n",
    "    10.001\n",
    ")\n",
    "\n",
    "genu_estimate = np.array([])\n",
    "genu_std = np.array([])\n",
    "splenium_estimate = np.array([])\n",
    "splenium_std = np.array([])\n",
    "deepgm_estimate = np.array([])\n",
    "deepgm_std = np.array([])\n",
    "cgm_estimate = np.array([])\n",
    "cgm_std = np.array([])\n",
    "\n",
    "ii = 0\n",
    "for index in ids_intersubmissions:\n",
    "    \n",
    "    genu_estimate = np.append(genu_estimate, np.mean(df.loc[index]['T1 - genu (WM)']))\n",
    "    splenium_estimate = np.append(splenium_estimate, np.mean(df.loc[index]['T1 - splenium (WM)']))\n",
    "    deepgm_estimate = np.append(deepgm_estimate, np.mean(df.loc[index]['T1 - deep GM']))\n",
    "    cgm_estimate = np.append(cgm_estimate, np.mean(df.loc[index]['T1 - cortical GM']))\n",
    "    \n",
    "    ii = ii +1\n",
    "\n",
    "stats_table['genu WM']['inter-submission mean T1'] = np.nanmean(genu_estimate)\n",
    "stats_table['genu WM']['inter-submission T1 STD'] = np.nanstd(genu_estimate)\n",
    "stats_table['genu WM']['inter-submission COV [%]'] = np.divide(np.nanstd(genu_estimate),np.nanmean(genu_estimate)) * 100\n",
    "\n",
    "stats_table['splenium WM']['inter-submission mean T1'] = np.nanmean(splenium_estimate)\n",
    "stats_table['splenium WM']['inter-submission T1 STD'] = np.nanstd(splenium_estimate)\n",
    "stats_table['splenium WM']['inter-submission COV [%]'] = np.divide(np.nanstd(splenium_estimate),np.nanmean(splenium_estimate)) * 100\n",
    "\n",
    "stats_table['cortical GM']['inter-submission mean T1'] = np.nanmean(cgm_estimate)\n",
    "stats_table['cortical GM']['inter-submission T1 STD'] = np.nanstd(cgm_estimate)\n",
    "stats_table['cortical GM']['inter-submission COV [%]'] = np.divide(np.nanstd(cgm_estimate),np.nanmean(cgm_estimate)) * 100\n",
    "\n",
    "stats_table['deep GM']['inter-submission mean T1'] = np.nanmean(deepgm_estimate)\n",
    "stats_table['deep GM']['inter-submission T1 STD'] = np.nanstd(deepgm_estimate)\n",
    "stats_table['deep GM']['inter-submission COV [%]'] = np.divide(np.nanstd(deepgm_estimate),np.nanmean(deepgm_estimate)) * 100\n",
    "\n",
    "genu_estimate_inter = genu_estimate\n",
    "splenium_estimate_inter = splenium_estimate\n",
    "deepgm_estimate_inter = deepgm_estimate\n",
    "cgm_estimate_inter = cgm_estimate\n",
    "\n",
    "## Intra-submission stats\n",
    "#All unique T1 maps from Site 9 were used for the stats calculations. Site 9 acquired maps on 3 different scanners, 2 different vendors.\n",
    "\n",
    "# Selecting one submission\n",
    "ids_intrasubmissions = (\n",
    "9.001,\n",
    "9.002,\n",
    "9.006,\n",
    "9.007,\n",
    "9.009,\n",
    "9.01,\n",
    "9.012,\n",
    "9.013,\n",
    "9.015,\n",
    "9.016,\n",
    "9.018,\n",
    "9.019,\n",
    "9.02,\n",
    "9.021,\n",
    "9.023,\n",
    "9.025,\n",
    "9.027,\n",
    "9.028,\n",
    "9.03,\n",
    "9.031,\n",
    "9.033,\n",
    "9.034,\n",
    "9.036,\n",
    "9.037\n",
    ")\n",
    "\n",
    "genu_estimate = np.array([])\n",
    "genu_std = np.array([])\n",
    "splenium_estimate = np.array([])\n",
    "splenium_std = np.array([])\n",
    "deepgm_estimate = np.array([])\n",
    "deepgm_std = np.array([])\n",
    "cgm_estimate = np.array([])\n",
    "cgm_std = np.array([])\n",
    "\n",
    "ii = 0\n",
    "for index in ids_intrasubmissions:\n",
    "    \n",
    "    genu_estimate = np.append(genu_estimate, np.mean(df.loc[index]['T1 - genu (WM)']))\n",
    "    splenium_estimate = np.append(splenium_estimate, np.mean(df.loc[index]['T1 - splenium (WM)']))\n",
    "    deepgm_estimate = np.append(deepgm_estimate, np.mean(df.loc[index]['T1 - deep GM']))\n",
    "    cgm_estimate = np.append(cgm_estimate, np.mean(df.loc[index]['T1 - cortical GM']))\n",
    "\n",
    "    ii = ii +1\n",
    "\n",
    "stats_table['genu WM']['intra-submission mean T1'] = np.nanmean(genu_estimate)\n",
    "stats_table['genu WM']['intra-submission T1 STD'] = np.nanstd(genu_estimate)\n",
    "stats_table['genu WM']['intra-submission COV [%]'] = np.divide(np.nanstd(genu_estimate),np.nanmean(genu_estimate)) * 100\n",
    "\n",
    "stats_table['splenium WM']['intra-submission mean T1'] = np.nanmean(splenium_estimate)\n",
    "stats_table['splenium WM']['intra-submission T1 STD'] = np.nanstd(splenium_estimate)\n",
    "stats_table['splenium WM']['intra-submission COV [%]'] = np.divide(np.nanstd(splenium_estimate),np.nanmean(splenium_estimate)) * 100\n",
    "\n",
    "stats_table['cortical GM']['intra-submission mean T1'] = np.nanmean(cgm_estimate)\n",
    "stats_table['cortical GM']['intra-submission T1 STD'] = np.nanstd(cgm_estimate)\n",
    "stats_table['cortical GM']['intra-submission COV [%]'] = np.divide(np.nanstd(cgm_estimate),np.nanmean(cgm_estimate)) * 100\n",
    "\n",
    "stats_table['deep GM']['intra-submission mean T1'] = np.nanmean(deepgm_estimate)\n",
    "stats_table['deep GM']['intra-submission T1 STD'] = np.nanstd(deepgm_estimate)\n",
    "stats_table['deep GM']['intra-submission COV [%]'] = np.divide(np.nanstd(deepgm_estimate),np.nanmean(deepgm_estimate)) * 100\n",
    "\n",
    "genu_estimate_intra = genu_estimate\n",
    "splenium_estimate_intra = splenium_estimate\n",
    "deepgm_estimate_intra = deepgm_estimate\n",
    "cgm_estimate_intra = cgm_estimate\n",
    "\n",
    "# Violin plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Combine data into a single DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'T1 (ms)': np.concatenate([genu_estimate_inter, genu_estimate_intra, splenium_estimate_inter, splenium_estimate_intra, cgm_estimate_inter, cgm_estimate_intra, deepgm_estimate_inter, deepgm_estimate_intra]),\n",
    "    'Group': ['Genu'] * len(genu_estimate_inter) + ['Genu'] * len(genu_estimate_intra) + ['Splenium'] * len(splenium_estimate_inter) + ['Splenium'] * len(splenium_estimate_intra) + ['Cgm'] * len(cgm_estimate_inter) + ['Cgm'] * len(cgm_estimate_intra) + ['DeepGM'] * len(deepgm_estimate_inter) + ['DeepGM'] * len(deepgm_estimate_intra),\n",
    "    'Type': ['Inter'] * len(genu_estimate_inter) + ['Intra'] * len(genu_estimate_intra) + ['Inter'] * len(splenium_estimate_inter) + ['Intra'] * len(splenium_estimate_intra) + ['Inter'] * len(cgm_estimate_inter) + ['Intra'] * len(cgm_estimate_intra) + ['Inter'] * len(deepgm_estimate_inter) + ['Intra'] * len(deepgm_estimate_intra),\n",
    "    'Region': ['Genu'] * len(genu_estimate_inter) + ['Genu'] * len(genu_estimate_intra) + ['Splenium'] * len(splenium_estimate_inter) + ['Splenium'] * len(splenium_estimate_intra) + ['Cgm'] * len(cgm_estimate_inter) + ['Cgm'] * len(cgm_estimate_intra) + ['DeepGM'] * len(deepgm_estimate_inter) + ['DeepGM'] * len(deepgm_estimate_intra),\n",
    "})\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from plotly import __version__\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "config={\n",
    "    'showLink': False,\n",
    "    'displayModeBar': False,\n",
    "    'toImageButtonOptions': {\n",
    "                'format': 'png', # one of png, svg, jpeg, webp\n",
    "                'filename': 'custom_image',\n",
    "                'height': 400,\n",
    "                'width': 800,\n",
    "                'scale': 2 # Multiply title/legend/axis/canvas sizes by this factor\n",
    "            }\n",
    "    }\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the color palette\n",
    "inter_color='rgb(113, 180, 159)'\n",
    "intra_color='rgb(233, 150, 117)'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "df1 = pd.DataFrame({\n",
    "    'T1 (ms)': np.concatenate([genu_estimate_inter, splenium_estimate_inter,  cgm_estimate_inter, deepgm_estimate_inter]),\n",
    "    'Group': ['Genu'] * len(genu_estimate_inter) + ['Splenium'] * len(splenium_estimate_inter) + ['Cgm'] * len(cgm_estimate_inter) + ['DeepGM'] * len(deepgm_estimate_inter) ,\n",
    "    'Type': ['Inter'] * len(genu_estimate_inter)  + ['Inter'] * len(splenium_estimate_inter)  + ['Inter'] * len(cgm_estimate_inter) + ['Inter'] * len(deepgm_estimate_inter) ,\n",
    "    'Region': ['<b>Genu</b>'] * len(genu_estimate_inter)  + ['<b>Splenium</b>'] * len(splenium_estimate_inter)  + ['<b>cGM</b>'] * len(cgm_estimate_inter)  + ['<b>Deep GM</b>'] * len(deepgm_estimate_inter) ,\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'T1 (ms)': np.concatenate([genu_estimate_intra, splenium_estimate_intra,  cgm_estimate_intra, deepgm_estimate_intra]),\n",
    "    'Group': ['Genu'] * len(genu_estimate_intra) + ['Splenium'] * len(splenium_estimate_intra) + ['Cgm'] * len(cgm_estimate_intra) + ['DeepGM'] * len(deepgm_estimate_intra) ,\n",
    "    'Type': ['Intra'] * len(genu_estimate_intra)  + ['Intra'] * len(splenium_estimate_intra)  + ['Intra'] * len(cgm_estimate_intra) + ['Intra'] * len(deepgm_estimate_intra) ,\n",
    "    'Region': ['<b>Genu</b>'] * len(genu_estimate_intra)  + ['<b>Splenium</b>'] * len(splenium_estimate_intra)  + ['<b>cGM</b>'] * len(cgm_estimate_intra)  + ['<b>Deep GM</b>'] * len(deepgm_estimate_intra) ,\n",
    "})\n",
    "\n",
    "\n",
    "fig.add_trace(go.Violin(x=df1['Region'],\n",
    "                        y=df1['T1 (ms)'],\n",
    "                        side='negative',\n",
    "                        line_color='black',\n",
    "                        fillcolor=inter_color, \n",
    "                        line=dict(width=1.5),\n",
    "                        points=False,\n",
    "                        width=1.15,\n",
    "                        showlegend=False\n",
    "                       )\n",
    "             )\n",
    "fig.add_trace(go.Violin(x=df2['Region'],\n",
    "                        y=df2['T1 (ms)'],\n",
    "                        side='positive',\n",
    "                        line_color='black',\n",
    "                        fillcolor=intra_color, \n",
    "                        line=dict(width=1.5),\n",
    "                        points=False,\n",
    "                        width=1.15,\n",
    "                        showlegend=False\n",
    "                    )\n",
    "             )\n",
    "fig.update_layout(violingap=0, violinmode='overlay')\n",
    "#fig.show()\n",
    "\n",
    "fig.update_xaxes(\n",
    "    type=\"category\",\n",
    "    autorange=False,\n",
    "    range=[-0.6,3.5],\n",
    "    dtick=1,\n",
    "    showgrid=False,\n",
    "    gridwidth =1,\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    tickfont=dict(\n",
    "        family='Times New Roman',\n",
    "        size=22,\n",
    "    ),\n",
    "    )\n",
    "fig.update_yaxes(\n",
    "    title='<b>T<sub>1</sub> (ms)</b>',\n",
    "    autorange=False,\n",
    "    range=[600, 2500],\n",
    "    showgrid=True,\n",
    "    dtick=250,\n",
    "    gridcolor='rgb(169,169,169)',\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    tickfont=dict(\n",
    "        family='Times New Roman',\n",
    "        size=22,\n",
    "    ),\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    margin=go.layout.Margin(\n",
    "        l=80,\n",
    "        r=40,\n",
    "        b=80,\n",
    "        t=10,\n",
    "    ),\n",
    "    font=dict(\n",
    "        family='Times New Roman',\n",
    "        size=22\n",
    "    ),\n",
    "   \n",
    "    paper_bgcolor='rgb(255, 255, 255)',\n",
    "    plot_bgcolor='rgb(255, 255, 255)',\n",
    "    annotations=[\n",
    "        dict(\n",
    "            x=-0.1,\n",
    "            y=-0.22,\n",
    "            showarrow=False,\n",
    "            text='<b>b</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=64\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# update characteristics shared by all traces\n",
    "fig.update_traces(quartilemethod='linear',\n",
    "                  scalemode='count',\n",
    "                  meanline=dict(visible=False),\n",
    "                  )\n",
    "\n",
    "#iplot(fig, filename = 'figure3', config = config)\n",
    "plot(fig, filename = 'figure3b.html', config = config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "report_output",
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "display(HTML('figure3a.html'))\n",
    "display(HTML('figure3b.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"caption\">\n",
    "<b>Figure 3</b> Summary of results of the challenge as violin plots displaying the inter- and intra- submission dataset comparisons for phantoms (a) and human brains (b). Green indicates submissions used for inter-submission analyses, and orange indicates the sites used for intra-submission analyses. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatterplot of the T<sub>1</sub> data for all submissions and their ROIs is shown in Figure 4 (phantom a-c, and human brains d-f). The NIST phantom T<sub>1</sub> measurements are presented in each plot for different axes types (linear, log, and error) to better visualize the results. Figure 4-a shows good agreement for this dataset in comparison with the temperature-corrected reference T<sub>1</sub> values. However, this trend did not persist for low T<sub>1</sub> values (T<sub>1</sub> < 100-200 ms), as seen in the log-log plot (Figure 4-b), which was expected because the imaging protocol is optimized for human brain T<sub>1</sub> values (T<sub>1</sub> > 500 ms). Higher variability is seen at long T<sub>1</sub> values (T<sub>1</sub> ~ 2000 ms) in Figure 4-a. Errors exceeding 10% are observed in the phantom spheres with T1 values below 300 ms (Figure 4-c), and 3-4 measurements with outlier values exceeding 10% error were observed in the human brain tissue range (~500-2000 ms).\n",
    "\n",
    "Figure 4 d-f displays the scatter plot data for human datasets submitted to this challenge, showing mean and standard deviation T<sub>1</sub> values for the WM (genu and splenium) and GM (cerebral cortex and deep GM) ROIs. Mean WM T<sub>1</sub> values across all submissions were 828 ± 38 ms in the genu and 852 ± 49 ms in the splenium, and mean GM T<sub>1</sub> values were 1548 ± 156 ms in the cortex and 1188 ± 133 ms in the deep GM, with less variations overall in WM compared to GM, possibly due to better ROI placement and less partial voluming in WM. The lower standard deviations for the ROIs of human database ID site 9 (by submission 18, Figure 1, and seen in orange in Figure 4d-g) are due to good slice positioning, cutting through the AC-PC line and the genu for proper ROI placement, particularly for the corpus callosum and deep GM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_output",
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "# PYTHON CODE\n",
    "# Module imports\n",
    "\n",
    "# Base python\n",
    "import os\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "import markdown\n",
    "import random\n",
    "\n",
    "# Graphical\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.colors\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display, HTML\n",
    "from plotly import __version__\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "config={\n",
    "    'showLink': False,\n",
    "    'displayModeBar': False,\n",
    "    'toImageButtonOptions': {\n",
    "                'format': 'png', # one of png, svg, jpeg, webp\n",
    "                'filename': 'custom_image',\n",
    "                'height': 300,\n",
    "                'width': 960,\n",
    "                'scale': 2 # Multiply title/legend/axis/canvas sizes by this factor\n",
    "            }\n",
    "    }\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the color palette\n",
    "pal=sns.color_palette(\"Set2\")\n",
    "\n",
    "# Scientific\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "import scipy.io\n",
    "\n",
    "# Data\n",
    "from repo2data.repo2data import Repo2Data\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "if build == 'latest':\n",
    "    if path.isdir('analysis')== False:\n",
    "        !git clone https://github.com/rrsg2020/analysis.git\n",
    "        dir_name = 'analysis'\n",
    "        analysis = os.listdir(dir_name)\n",
    "\n",
    "        for item in analysis:\n",
    "            if item.endswith(\".ipynb\"):\n",
    "                os.remove(os.path.join(dir_name, item))\n",
    "            if item.endswith(\".md\"):\n",
    "                os.remove(os.path.join(dir_name, item))\n",
    "elif build == 'archive':\n",
    "    if os.path.isdir(Path('../data')):\n",
    "        data_path = ['../data/rrsg-2020-note/rrsg-2020-neurolibre']\n",
    "    else:\n",
    "        # define data requirement path\n",
    "        data_req_path = os.path.join(\"..\", \"binder\", \"data_requirement.json\")\n",
    "        # download data\n",
    "        repo2data = Repo2Data(data_req_path)\n",
    "        data_path = repo2data.install()\n",
    "\n",
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "from analysis.src.database import *\n",
    "from analysis.src.nist import get_reference_NIST_values, get_NIST_ids\n",
    "from analysis.src.tools import calc_error\n",
    "from analysis.src.nist import temperature_correction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('analysis/custom_matplotlibrc')\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "fig_id = 0\n",
    "\n",
    "if build == 'latest':\n",
    "    database_path = Path('analysis/databases/3T_NIST_T1maps_database.pkl')\n",
    "    output_folder = Path(\"analysis/plots/03_singledataset_scatter_NIST-temperature-corrected/\")\n",
    "elif build=='archive':\n",
    "    database_path = Path(data_path[0] + '/analysis/databases/3T_NIST_T1maps_database.pkl')\n",
    "    output_folder = Path(data_path[0] + '/analysis/plots/03_singledataset_scatter_NIST-temperature-corrected/')\n",
    "\n",
    "estimate_type = 'mean' # median or mean\n",
    "\n",
    "## Define Functions\n",
    "def plot_single_scatter(x, y, y_std,\n",
    "                        title, x_label, y_label,\n",
    "                        file_prefix, folder_path, fig_id,\n",
    "                        y_type='linear'):\n",
    "    if y_type == 'linear':\n",
    "        plt.errorbar(x,y, y_std, fmt='o', solid_capstyle='projecting')\n",
    "        ax = plt.gca()\n",
    "        ax.axline((1, 1), slope=1, linestyle='dashed')\n",
    "        ax.set_ylim(ymin=0, ymax=2500)\n",
    "        ax.set_xlim(xmin=0, xmax=2500)\n",
    "    if y_type == 'log':\n",
    "        plt.loglog(x,y,'o')\n",
    "        ax = plt.gca()\n",
    "        ax.set_ylim(ymin=20, ymax=2500)\n",
    "        ax.set_xlim(xmin=20, xmax=2500)\n",
    "    if y_type == 'error_t1':\n",
    "        plt.errorbar(x,calc_error(x,y), fmt='o')\n",
    "        ax = plt.gca()\n",
    "        ax.axline((1, 0), slope=0, color='k')\n",
    "        ax.axline((1, -10), slope=0, linestyle='dashed', color='k')\n",
    "        ax.axline((1, 10), slope=0, linestyle='dashed', color='k')\n",
    "        ax.set_ylim(ymin=-100, ymax=100)\n",
    "        ax.set_xlim(xmin=0, xmax=2500)\n",
    "\n",
    "\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    \n",
    "\n",
    "    folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if fig_id<10:\n",
    "        filename = \"0\" + str(fig_id) + \"_\" + file_prefix\n",
    "    else:\n",
    "        filename = str(fig_id) + \"_\" + file_prefix\n",
    "\n",
    "    fig.savefig(folder_path / (str(filename) + '.svg'), facecolor='white')\n",
    "    fig.savefig(folder_path / (str(filename) + '.png'), facecolor='white')\n",
    "    fig_id = fig_id + 1\n",
    "    plt.show()\n",
    "    return fig_id\n",
    "\n",
    "## Load database\n",
    "\n",
    "df = pd.read_pickle(database_path)\n",
    "\n",
    "\n",
    "## Initialize array\n",
    "\n",
    "dataset_mean = np.zeros((1,14))\n",
    "dataset_std = np.zeros((1,14))\n",
    "version = np.array([])\n",
    "temperature = np.array([])\n",
    "ref_values = np.zeros((1,14))\n",
    "\n",
    "intra_bool = np.array([])\n",
    "ii=0\n",
    "for index, row in df.iterrows():\n",
    "    intra_bool = np.append(intra_bool, round(index)==6)\n",
    "    if type(df.loc[index]['T1 - NIST sphere 1']) is np.ndarray:\n",
    "\n",
    "        version = np.append(version,df.loc[index]['phantom serial number'])\n",
    "        temperature = np.append(temperature, df.loc[index]['phantom temperature'])\n",
    "\n",
    "\n",
    "        if version[ii] == None:\n",
    "            version[ii] = 999 # Missing version, only known case is one where we have version > 42 right now.\n",
    "        \n",
    "        if temperature[ii] == None:\n",
    "            temperature[ii] = 20 # Missing temperature, assume it to be 20C (reference temperature).\n",
    "            \n",
    "            \n",
    "        if ii==0:\n",
    "            ref_values = get_reference_NIST_values(version[ii])\n",
    "            temp_corrected_ref_values = temperature_correction(temperature[ii], version[ii])\n",
    "        else:\n",
    "            ref_values = np.vstack((ref_values, get_reference_NIST_values(version[ii])))\n",
    "            temp_corrected_ref_values = np.vstack((temp_corrected_ref_values, temperature_correction(temperature[ii], version[ii])))\n",
    "        \n",
    "        tmp_dataset_estimate = np.array([])\n",
    "        tmp_dataset_std = np.array([])\n",
    "\n",
    "        for key in get_NIST_ids():\n",
    "            if estimate_type == 'mean':\n",
    "                tmp_dataset_estimate = np.append(tmp_dataset_estimate, np.mean(df.loc[index][key]))\n",
    "            elif estimate_type == 'median':\n",
    "                tmp_dataset_estimate = np.append(tmp_dataset_estimate, np.median(df.loc[index][key]))\n",
    "            else:\n",
    "                Exception('Unsupported dataset estimate type.')\n",
    "\n",
    "            tmp_dataset_std = np.append(tmp_dataset_std, np.std(df.loc[index][key]))\n",
    "\n",
    "        if ii==0:\n",
    "            dataset_estimate = tmp_dataset_estimate  \n",
    "            dataset_std = tmp_dataset_std\n",
    "        else:\n",
    "            dataset_estimate = np.vstack((dataset_estimate, tmp_dataset_estimate))\n",
    "            dataset_std = np.vstack((dataset_std, tmp_dataset_std))\n",
    "\n",
    "        ii=ii+1\n",
    "\n",
    "## Setup for plots\n",
    "fig_id = 0\n",
    "dims=ref_values.shape\n",
    "file_prefix = 'alldatasets'\n",
    "fig = make_subplots(rows=1, cols=3, horizontal_spacing = 0.08)\n",
    "\n",
    "data_lin_intra=[]\n",
    "data_lin_inter=[]\n",
    "for ii in range(dims[0]):\n",
    "    if bool(intra_bool[ii]):\n",
    "        marker_color='rgba(252, 141, 98, 0.5)'\n",
    "        marker_zorder=1\n",
    "    else:\n",
    "        marker_color='rgba(102, 194, 165, 0.5)'\n",
    "        marker_zorder=-1\n",
    "        \n",
    "    data_lin=go.Scatter(\n",
    "        x=temp_corrected_ref_values[ii,:],\n",
    "        y=dataset_estimate[ii,:],\n",
    "        error_y=dict(\n",
    "            type='data', # value of error bar given in data coordinates\n",
    "            array=dataset_std[ii,:],\n",
    "            visible=True),\n",
    "        name = 'id: '+ str(df.index[ii]),\n",
    "        mode = 'markers',\n",
    "        visible = True,\n",
    "        showlegend = False,\n",
    "        marker=dict(\n",
    "            color=marker_color\n",
    "            )\n",
    "        )\n",
    "    # For z-ordering   \n",
    "    if bool(intra_bool[ii]):\n",
    "        data_lin_intra.append(data_lin)\n",
    "    else:\n",
    "        data_lin_inter.append(data_lin)\n",
    "\n",
    "# For z-ordering   \n",
    "for trace in data_lin_inter:\n",
    "    fig.add_trace(\n",
    "        trace,\n",
    "        row=1, col=1\n",
    "    )\n",
    "for trace in data_lin_intra:\n",
    "    fig.add_trace(\n",
    "        trace,\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "data_log_intra=[]\n",
    "data_log_inter=[]\n",
    "for ii in range(dims[0]):\n",
    "    if bool(intra_bool[ii]):\n",
    "        marker_color='rgba(252, 141, 98, 0.5)'\n",
    "        marker_zorder=1\n",
    "    else:\n",
    "        marker_color='rgba(102, 194, 165, 0.5)'\n",
    "        marker_zorder=-1\n",
    "        \n",
    "    data_log=go.Scatter(\n",
    "        x=temp_corrected_ref_values[ii,:],\n",
    "        y=dataset_estimate[ii,:],\n",
    "        name = 'id: '+ str(df.index[ii]),\n",
    "        mode = 'markers',\n",
    "        visible = True,\n",
    "        showlegend = False,\n",
    "        marker=dict(\n",
    "            color=marker_color\n",
    "            )\n",
    "        )\n",
    "    # For z-ordering   \n",
    "    if bool(intra_bool[ii]):\n",
    "        data_log_intra.append(data_log)\n",
    "    else:\n",
    "        data_log_inter.append(data_log)\n",
    "\n",
    "# For z-ordering   \n",
    "for trace in data_log_inter:\n",
    "    fig.add_trace(\n",
    "        trace,\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "for trace in data_log_intra:\n",
    "    fig.add_trace(\n",
    "        trace,\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "data_error_intra=[]\n",
    "data_error_inter=[]\n",
    "for ii in range(dims[0]):\n",
    "    if bool(intra_bool[ii]):\n",
    "        marker_color='rgba(252, 141, 98, 0.5)'\n",
    "        marker_zorder=1\n",
    "    else:\n",
    "        marker_color='rgba(102, 194, 165, 0.5)'\n",
    "        marker_zorder=-1\n",
    "\n",
    "    data_error=go.Scatter(\n",
    "        x=temp_corrected_ref_values[ii,:],\n",
    "        y= calc_error(temp_corrected_ref_values[ii,:],dataset_estimate[ii,:]),\n",
    "        name = 'id: '+ str(df.index[ii]),\n",
    "        mode = 'markers',\n",
    "        visible = True,\n",
    "        showlegend = False,\n",
    "        marker=dict(\n",
    "            color=marker_color\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # For z-ordering   \n",
    "    if bool(intra_bool[ii]):\n",
    "        data_error_intra.append(data_error)\n",
    "    else:\n",
    "        data_error_inter.append(data_error)\n",
    "\n",
    "# For z-ordering   \n",
    "for trace in data_error_inter:\n",
    "    fig.add_trace(\n",
    "        trace,\n",
    "        row=1, col=3\n",
    "    )\n",
    "for trace in data_error_intra:\n",
    "    fig.add_trace(\n",
    "        trace,\n",
    "        row=1, col=3\n",
    "    )\n",
    "    \n",
    "fig.update_xaxes(\n",
    "    type=\"linear\",\n",
    "    range=[0,2500],\n",
    "    title='<b>Reference T<sub>1</sub> (ms)</b>',\n",
    "    showgrid=True,\n",
    "    gridcolor='rgb(169,169,169)',\n",
    "    tick0 = 0,\n",
    "    dtick = 500,\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    title_font_size = 20,\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    row=1, col=1\n",
    "    )\n",
    "fig.update_yaxes(\n",
    "    type=\"linear\",\n",
    "    range=[0,2500],\n",
    "    title={\n",
    "        'text':'<b>T<sub>1</sub> estimate (ms)</b>',\n",
    "        'standoff':0\n",
    "        },\n",
    "    showgrid=True,\n",
    "    gridcolor='rgb(169,169,169)',\n",
    "    tick0 = 0,\n",
    "    dtick = 500,\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    title_font_size = 20,\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(\n",
    "    type=\"log\",\n",
    "    range=[np.log10(20),np.log10(2500)],\n",
    "    title='<b>Reference T<sub>1</sub> (ms)</b>',\n",
    "    showgrid=True,\n",
    "    gridcolor='rgb(169,169,169)',\n",
    "    minor=dict(ticks=\"inside\", ticklen=6, showgrid=True),\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    title_font_size = 20,\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_yaxes(\n",
    "    type=\"log\",\n",
    "    range=[np.log10(20),np.log10(2500)],\n",
    "    title={\n",
    "        'text':'<b>T<sub>1</sub> estimate (ms)</b>',\n",
    "        'standoff':0\n",
    "        },\n",
    "    showgrid=True,\n",
    "    gridcolor='rgb(169,169,169)',\n",
    "    minor=dict(ticks=\"inside\", ticklen=6, showgrid=True),\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    title_font_size = 20,\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(\n",
    "    type=\"linear\",\n",
    "    range=[0,2500],\n",
    "    title='<b>Reference T<sub>1</sub> (ms)</b>',\n",
    "    showgrid=True,\n",
    "    gridcolor='rgb(169,169,169)',\n",
    "    tick0 = 0,\n",
    "    dtick = 500,\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    title_font_size = 20,\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    row=1, col=3)\n",
    "\n",
    "fig.update_yaxes(\n",
    "    type=\"linear\",\n",
    "    range=[-100,100],\n",
    "    title={\n",
    "        'text':'<b>Error (%)</b>',\n",
    "        'standoff':0\n",
    "        },\n",
    "    showgrid=True,\n",
    "    gridcolor='rgb(169,169,169)',\n",
    "    tick0 = -100,\n",
    "    dtick = 25,\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    title_font_size = 20,\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    row=1, col=3,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    margin=dict(l=30, r=30, t=10, b=30),\n",
    "    paper_bgcolor='rgb(255, 255, 255)',\n",
    "    plot_bgcolor='rgb(255, 255, 255)',\n",
    "    legend_title=\"\",\n",
    "    annotations=[\n",
    "        dict(\n",
    "            x=-0.05,\n",
    "            y=-0.22,\n",
    "            showarrow=False,\n",
    "            text='<b>a</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=64\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "        ),\n",
    "        dict(\n",
    "            x=0.3,\n",
    "            y=-0.22,\n",
    "            showarrow=False,\n",
    "            text='<b>b</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=64\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "        ),\n",
    "        dict(\n",
    "            x=0.68,\n",
    "            y=-0.22,\n",
    "            showarrow=False,\n",
    "            text='<b>c</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=64\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "        ),\n",
    "        dict(\n",
    "            x=1,\n",
    "            y=0.99,\n",
    "            showarrow=False,\n",
    "            text='<b>Inter-sites</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=20,\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper',\n",
    "            bgcolor='rgba(102, 194, 165, 0.8)'\n",
    "        ),\n",
    "        dict(\n",
    "            x=1,\n",
    "            y=0.871,\n",
    "            showarrow=False,\n",
    "            text='<b>Intra-sites</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=20,\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper',\n",
    "            bgcolor='rgba(252, 141, 98, 0.8)'\n",
    "        ),        \n",
    "    ]\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=300, width=960)\n",
    "\n",
    "#iplot(fig, filename = 'figure4a', config = config)\n",
    "plot(fig, filename = 'figure4a.html', config = config)\n",
    "\n",
    "from os import path\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "if build == 'latest':\n",
    "    if path.isdir('analysis')== False:\n",
    "        !git clone https://github.com/rrsg2020/analysis.git\n",
    "        dir_name = 'analysis'\n",
    "        analysis = os.listdir(dir_name)\n",
    "\n",
    "        for item in analysis:\n",
    "            if item.endswith(\".ipynb\"):\n",
    "                os.remove(os.path.join(dir_name, item))\n",
    "            if item.endswith(\".md\"):\n",
    "                os.remove(os.path.join(dir_name, item))\n",
    "elif build == 'archive':\n",
    "    if os.path.isdir(Path('../data')):\n",
    "        data_path = ['../data/rrsg-2020-note/rrsg-2020-neurolibre']\n",
    "    else:\n",
    "        # define data requirement path\n",
    "        data_req_path = os.path.join(\"..\", \"binder\", \"data_requirement.json\")\n",
    "        # download data\n",
    "        repo2data = Repo2Data(data_req_path)\n",
    "        data_path = repo2data.install()         \n",
    "\n",
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "from analysis.src.database import *\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('analysis/custom_matplotlibrc')\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "fig_id = 0\n",
    "\n",
    "# Configurations\n",
    "if build == 'latest':\n",
    "    database_path = Path('analysis/databases/3T_human_T1maps_database.pkl')\n",
    "    output_folder = Path(\"analysis/plots/08_wholedataset_scatter_Human/\")\n",
    "elif build=='archive':\n",
    "    database_path = Path(data_path[0] + '/analysis/databases/3T_human_T1maps_database.pkl')\n",
    "    output_folder = Path(data_path[0] + '/analysis/plots/08_wholedataset_scatter_Human/')\n",
    "\n",
    "estimate_type = 'mean' # median or mean\n",
    "\n",
    "# Load database\n",
    "\n",
    "df = pd.read_pickle(database_path)\n",
    "\n",
    "genu_estimate = np.array([])\n",
    "genu_std = np.array([])\n",
    "splenium_estimate = np.array([])\n",
    "splenium_std = np.array([])\n",
    "deepgm_estimate = np.array([])\n",
    "deepgm_std = np.array([])\n",
    "cgm_estimate = np.array([])\n",
    "cgm_std = np.array([])\n",
    "\n",
    "intra_bool = np.array([])\n",
    "ii = 0\n",
    "for index, row in df.iterrows():\n",
    "    intra_bool = np.append(intra_bool, round(index)==9)  \n",
    "    if estimate_type == 'mean':\n",
    "        genu_estimate = np.append(genu_estimate, np.mean(df.loc[index]['T1 - genu (WM)']))\n",
    "        genu_std = np.append(genu_std, np.std(df.loc[index]['T1 - genu (WM)']))\n",
    "        splenium_estimate = np.append(splenium_estimate, np.mean(df.loc[index]['T1 - splenium (WM)']))\n",
    "        splenium_std = np.append(splenium_std, np.std(df.loc[index]['T1 - splenium (WM)']))\n",
    "        deepgm_estimate = np.append(deepgm_estimate, np.mean(df.loc[index]['T1 - deep GM']))\n",
    "        deepgm_std = np.append(deepgm_std, np.std(df.loc[index]['T1 - deep GM']))\n",
    "        cgm_estimate = np.append(cgm_estimate, np.mean(df.loc[index]['T1 - cortical GM']))\n",
    "        cgm_std = np.append(cgm_std, np.std(df.loc[index]['T1 - cortical GM']))\n",
    "    elif estimate_type == 'median':\n",
    "        genu_estimate = np.append(genu_estimate, np.median(df.loc[index]['T1 - genu (WM)']))\n",
    "        genu_std = np.append(genu_std, np.std(df.loc[index]['T1 - genu (WM)']))\n",
    "        splenium_estimate = np.append(splenium_estimate, np.median(df.loc[index]['T1 - splenium (WM)']))\n",
    "        splenium_std = np.append(splenium_std, np.std(df.loc[index]['T1 - splenium (WM)']))\n",
    "        deepgm_estimate = np.append(deepgm_estimate, np.median(df.loc[index]['T1 - deep GM']))\n",
    "        deepgm_std = np.append(deepgm_std, np.std(df.loc[index]['T1 - deep GM']))\n",
    "        cgm_estimate = np.append(cgm_estimate, np.median(df.loc[index]['T1 - cortical GM']))\n",
    "        cgm_std = np.append(cgm_std, np.std(df.loc[index]['T1 - cortical GM']))\n",
    "    else:\n",
    "        Exception('Unsupported dataset estimate type.')\n",
    "    ii = ii +1\n",
    "\n",
    "# Store the IDs\n",
    "indexes_numbers = df.index\n",
    "indexes_strings = indexes_numbers.map(str)\n",
    "\n",
    "x1_label='Site #'\n",
    "x2_label='<b>Site #.Meas #</b>'\n",
    "y_label=\"<b>T$_1$ (ms)</b>\"\n",
    "file_prefix = 'WM_and_GM'\n",
    "folder_path=output_folder\n",
    "\n",
    "x1=indexes_numbers\n",
    "x2=indexes_strings\n",
    "y=genu_estimate\n",
    "y_std=genu_std\n",
    "\n",
    "# Paper formatting of x tick labels (remove leading zero, pad zero at the end for multiples of 10)\n",
    "x3=[]\n",
    "for num in x2:\n",
    "    x3.append(num.replace('.0', '.'))\n",
    "\n",
    "index=0\n",
    "for num in x3:\n",
    "    if num[-3] != '.':\n",
    "        x3[index]=num+'0'\n",
    "    index+=1\n",
    "\n",
    "# PYTHON CODE\n",
    "# Module imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.image import imread\n",
    "import scipy.io\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "from plotly import __version__\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "config={\n",
    "    'showLink': False,\n",
    "    'displayModeBar': False,\n",
    "    'toImageButtonOptions': {\n",
    "                'format': 'png', # one of png, svg, jpeg, webp\n",
    "                'filename': 'custom_image',\n",
    "                'height': 800,\n",
    "                'width': 960,\n",
    "                'scale': 2 # Multiply title/legend/axis/canvas sizes by this factor\n",
    "            }\n",
    "    }\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import os\n",
    "import markdown\n",
    "import random\n",
    "from scipy.integrate import quad\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "config={\n",
    "    'showLink': False,\n",
    "    'displayModeBar': False,\n",
    "    'toImageButtonOptions': {\n",
    "                'format': 'png', # one of png, svg, jpeg, webp\n",
    "                'filename': 'custom_image',\n",
    "                'height': 800,\n",
    "                'width': 960,\n",
    "                'scale': 2 # Multiply title/legend/axis/canvas sizes by this factor\n",
    "            }\n",
    "    }\n",
    "\n",
    "\n",
    "dims=genu_estimate.shape\n",
    "fig = make_subplots(rows=4, cols=1, vertical_spacing = 0.07)\n",
    "\n",
    "#data_genu=go.Scatter(\n",
    "#    x=x3,\n",
    "#   y=genu_estimate,\n",
    "#    error_y=dict(\n",
    "#        type='data', # value of error bar given in data coordinates\n",
    "#        array=genu_std,\n",
    "#        visible=True),\n",
    "#    mode = 'markers',\n",
    "#    marker=dict(color='#007ea7'),\n",
    "#    visible = True,\n",
    "#    showlegend = False\n",
    "#\n",
    "#    )\n",
    "\n",
    "\n",
    "# Genu\n",
    "for ii in range(dims[0]):\n",
    "    if bool(intra_bool[ii]):\n",
    "        marker_color='rgb(252, 141, 98)'\n",
    "    else:\n",
    "        marker_color='rgb(102, 194, 165)'\n",
    "        \n",
    "    data=go.Scatter(\n",
    "        x=[x3[ii]],\n",
    "        y=[genu_estimate[ii]],\n",
    "        error_y=dict(\n",
    "            type='data', # value of error bar given in data coordinates\n",
    "            array=[genu_std[ii]],\n",
    "            visible=True),\n",
    "        name = 'id: '+ str(df.index[ii]),\n",
    "        mode = 'markers',\n",
    "        visible = True,\n",
    "        showlegend = False,\n",
    "        marker=dict(\n",
    "            color=marker_color\n",
    "            )\n",
    "        )\n",
    "    fig.add_trace(\n",
    "        data,\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "# Splenium\n",
    "for ii in range(dims[0]):\n",
    "    if bool(intra_bool[ii]):\n",
    "        marker_color='rgb(252, 141, 98)'\n",
    "    else:\n",
    "        marker_color='rgb(102, 194, 165)'\n",
    "        \n",
    "    data=go.Scatter(\n",
    "        x=[x3[ii]],\n",
    "        y=[splenium_estimate[ii]],\n",
    "        error_y=dict(\n",
    "            type='data', # value of error bar given in data coordinates\n",
    "            array=[splenium_std[ii]],\n",
    "            visible=True),\n",
    "        name = 'id: '+ str(df.index[ii]),\n",
    "        mode = 'markers',\n",
    "        visible = True,\n",
    "        showlegend = False,\n",
    "        marker=dict(\n",
    "            color=marker_color\n",
    "            )\n",
    "        )\n",
    "    fig.add_trace(\n",
    "        data,\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# cGM\n",
    "for ii in range(dims[0]):\n",
    "    if bool(intra_bool[ii]):\n",
    "        marker_color='rgb(252, 141, 98)'\n",
    "    else:\n",
    "        marker_color='rgb(102, 194, 165)'\n",
    "        \n",
    "    data=go.Scatter(\n",
    "        x=[x3[ii]],\n",
    "        y=[cgm_estimate[ii]],\n",
    "        error_y=dict(\n",
    "            type='data', # value of error bar given in data coordinates\n",
    "            array=[cgm_std[ii]],\n",
    "            visible=True),\n",
    "        name = 'id: '+ str(df.index[ii]),\n",
    "        mode = 'markers',\n",
    "        visible = True,\n",
    "        showlegend = False,\n",
    "        marker=dict(\n",
    "            color=marker_color\n",
    "            )\n",
    "        )\n",
    "    fig.add_trace(\n",
    "        data,\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "# DeepGM\n",
    "for ii in range(dims[0]):\n",
    "    if bool(intra_bool[ii]):\n",
    "        marker_color='rgb(252, 141, 98)'\n",
    "    else:\n",
    "        marker_color='rgb(102, 194, 165)'\n",
    "        \n",
    "    data=go.Scatter(\n",
    "        x=[x3[ii]],\n",
    "        y=[deepgm_estimate[ii]],\n",
    "        error_y=dict(\n",
    "            type='data', # value of error bar given in data coordinates\n",
    "            array=[deepgm_std[ii]],\n",
    "            visible=True),\n",
    "        name = 'id: '+ str(df.index[ii]),\n",
    "        mode = 'markers',\n",
    "        visible = True,\n",
    "        showlegend = False,\n",
    "        marker=dict(\n",
    "            color=marker_color\n",
    "            )\n",
    "        )\n",
    "    fig.add_trace(\n",
    "        data,\n",
    "        row=4, col=1\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(\n",
    "    autorange=False,\n",
    "    range=[0,57],\n",
    "    showgrid=False,\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    tickangle = -90,\n",
    "    tickmode='linear',\n",
    "    tickfont=dict(\n",
    "        family='Times New Roman',\n",
    "        size=12,\n",
    "    ),\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    row=1, col=1\n",
    "    )\n",
    "fig.update_yaxes(\n",
    "    title='<b>T<sub>1</sub> (ms)</b>',\n",
    "    autorange=True,\n",
    "    showgrid=True,\n",
    "    dtick=100,\n",
    "    gridcolor='rgb(169,169,169)',\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    tickfont=dict(\n",
    "        family='Times New Roman',\n",
    "        size=18,\n",
    "    ),\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(\n",
    "    autorange=False,\n",
    "    range=[0,57],\n",
    "    showgrid=False,\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    tickangle = -90,\n",
    "    tickmode='linear',\n",
    "    tickfont=dict(\n",
    "        family='Times New Roman',\n",
    "        size=12,\n",
    "    ),\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    row=2, col=1\n",
    "    )\n",
    "fig.update_yaxes(\n",
    "    title='<b>T<sub>1</sub> (ms)</b>',\n",
    "    autorange=True,\n",
    "    showgrid=True,\n",
    "    dtick=100,\n",
    "    gridcolor='rgb(169,169,169)',\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    tickfont=dict(\n",
    "        family='Times New Roman',\n",
    "        size=18,\n",
    "    ),\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    row=2, col=1\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(\n",
    "    autorange=False,\n",
    "    range=[0,57],\n",
    "    showgrid=False,\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    tickangle = -90,\n",
    "    tickmode='linear',\n",
    "    tickfont=dict(\n",
    "        family='Times New Roman',\n",
    "        size=12,\n",
    "    ),\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    row=3, col=1\n",
    "    )\n",
    "fig.update_yaxes(\n",
    "    title='<b>T<sub>1</sub> (ms)</b>',\n",
    "    autorange=True,\n",
    "    showgrid=True,\n",
    "    dtick=250,\n",
    "    gridcolor='rgb(169,169,169)',\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    tickfont=dict(\n",
    "        family='Times New Roman',\n",
    "        size=18,\n",
    "    ),\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    row=3, col=1\n",
    "    )\n",
    "\n",
    "fig.update_xaxes(\n",
    "    title='<b>Site #.Meas #</b>',\n",
    "    autorange=False,\n",
    "    range=[0,57],\n",
    "    showgrid=False,\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    tickangle = -90,\n",
    "    tickmode='linear',\n",
    "    tickfont=dict(\n",
    "        family='Times New Roman',\n",
    "        size=12,\n",
    "    ),\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    row=4, col=1\n",
    "    )\n",
    "fig.update_yaxes(\n",
    "    title='<b>T<sub>1</sub> (ms)</b>',\n",
    "    autorange=True,\n",
    "    showgrid=True,\n",
    "    dtick=250,\n",
    "    gridcolor='rgb(169,169,169)',\n",
    "    linecolor='black',\n",
    "    linewidth=2,\n",
    "    tickfont=dict(\n",
    "        family='Times New Roman',\n",
    "        size=18,\n",
    "    ),\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    row=4, col=1\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    width=960,\n",
    "    height=800,\n",
    "    margin=go.layout.Margin(\n",
    "        l=80,\n",
    "        r=40,\n",
    "        b=80,\n",
    "        t=10,\n",
    "    ),\n",
    "    font=dict(\n",
    "        family='Times New Roman',\n",
    "        size=22\n",
    "    ),\n",
    "    annotations=[\n",
    "        dict(\n",
    "            x=-0.1,\n",
    "            y=0.85,\n",
    "            showarrow=False,\n",
    "            text='<b>d</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=64\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "        ),\n",
    "        dict(\n",
    "            x=-0.1,\n",
    "            y=0.55,\n",
    "            showarrow=False,\n",
    "            text='<b>e</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=64\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "            ),   \n",
    "        dict(\n",
    "            x=-0.1,\n",
    "            y=0.2,\n",
    "            showarrow=False,\n",
    "            text='<b>f</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=64\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "            ),   \n",
    "        dict(\n",
    "            x=-0.1,\n",
    "            y=-0.1,\n",
    "            showarrow=False,\n",
    "            text='<b>g</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=64\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "            ),   \n",
    "        dict(\n",
    "            x=1,\n",
    "            y=0.986,\n",
    "            showarrow=False,\n",
    "            text='<b>WM - Genu</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=20\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "            ),\n",
    "        dict(\n",
    "            x=1,\n",
    "            y=0.726,\n",
    "            showarrow=False,\n",
    "            text='<b>WM - Splenium</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=20\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "            ),\n",
    "        dict(\n",
    "            x=1,\n",
    "            y=0.425,\n",
    "            showarrow=False,\n",
    "            text='<b>cGM</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=20\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "            ),\n",
    "        dict(\n",
    "            x=1,\n",
    "            y=0.143,\n",
    "            showarrow=False,\n",
    "            text='<b>Deep GM</b>',\n",
    "            font=dict(\n",
    "                family='Times New Roman',\n",
    "                size=20\n",
    "            ),\n",
    "            xref='paper',\n",
    "            yref='paper'\n",
    "            ),\n",
    "        ],\n",
    "        paper_bgcolor='rgb(255, 255, 255)',\n",
    "        plot_bgcolor='rgb(255, 255, 255)',\n",
    ")\n",
    "\n",
    "#iplot(fig, filename = 'figure6b', config = config)\n",
    "plot(fig, filename = 'figure4d.html', config = config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "report_output",
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "display(HTML('figure4a.html'))\n",
    "display(HTML('figure4d.html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"caption\">\n",
    "<b>Figure 4</b>  Measured mean T<sub>1</sub> values vs. temperature-corrected NIST reference values of the phantom spheres are presented as linear plots (a), log-log plots (b), and plots of the error relative to the reference T<sub>1</sub> value (c). Green indicates submissions used for inter-submission analyses, and orange indicates the sites used for intra-submission analyses. The dashed lines in plots (c) represent a ±10 % error. Mean T<sub>1</sub> values in two sets of ROIs, white matter (one 5⨯5 voxel ROI for genu, one 5x5 voxel ROI for splenium) and gray matter (three 3⨯3 voxel ROIs for cortex, one 5x5 voxel ROI for deep GM). In subplot g), the missing datapoints for deep GM for sites 1, 8, and 10 were due to the slice positioning of the acquisition not containing deep GM.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 &nbsp; &nbsp; | &nbsp; &nbsp; DISCUSSION\n",
    "\n",
    "This challenge focused on exploring if different research groups could reproduce T<sub>1</sub> maps based on the protocol information reported in a seminal publication {cite:p}`Barral2010-qm`. Eighteen submissions independently implemented the inversion recovery T<sub>1</sub> mapping acquisition protocol as outlined in Barral et al. {cite:p}`Barral2010-qm`, and reported T<sub>1</sub> mapping data in a standard quantitative MRI phantom and/or human brains at 27 MRI sites, using systems from three different vendors (GE, Philips, Siemens). The collaborative effort produced an open-source database of 94 T<sub>1</sub> mapping datasets, including 38 ISMRM/NIST phantom and 56 human brain datasets. The inter-submission variability was twice as high as the intra-submission variability in both phantom and human brain T<sub>1</sub> measurements, **demonstrating that written instructions communicated via a PDF are not enough for reproducibility in quantitative MRI**. This study reports the inherent uncertainty in T<sub>1</sub> measures across independent research groups, which brings us one step closer to producing a practical baseline of variations for this metric.\n",
    "\n",
    "Overall, our approach did show improvement in the reproducibility of T<sub>1</sub> measurements in vivo compared to researchers implementing T<sub>1</sub> mapping protocols completely independently (i.e. with no central guidance), as literature T<sub>1</sub> values in vivo vary more than reported here (e.g., Bojorquez et al. {cite:p}`Bojorquez2017-xh` reports that reported T<sub>1</sub> values in WM vary between 699 to 1735 ms in published literature). We were aware that coordination was essential for a quantitative MRI challenge, which is why the protocol specifications we provided to researchers were more detailed than any guidelines for quantitative MR imaging that were available at the time. Yet, even in combination with the same T<sub>1</sub> mapping processing tools, this level of description (a PDF + post-processing tools) leaves something to be desired. \n",
    "\n",
    "This analysis highlights that more information is needed to unify all the aspects of a pulse sequence across sites, beyond what is routinely reported in a scientific publication. However, in a vendor-specific setting, this is a major challenge given the disparities between proprietary development libraries {cite:p}`Gracien2020-ak`. Vendor-neutral pulse sequence design platforms {cite:p}`Layton2017-dy,Cordes2020-au,Karakuzu2022-af` have emerged as a powerful solution to standardize sequence components at the implementation level (e.g., EF pulse shape, gradients, etc.). Vendor neutrality has been shown to significantly reduce the variability of T<sub>1</sub> maps acquired using VFA across vendors {cite:p}`Karakuzu2022-af`. In the absence of a vendor-neutral framework, a vendor-specific alternative is the implementation of a strategy to control the saturation of MT across TRs {cite:p}`A_G_Teixeira2020-bw`. Nevertheless, this approach can still benefit from a vendor-neutral protocol to enhance accessibility and unify implementations. This is because vendor-specific constraints are recognized to impose limitations on the adaptability of sequences, resulting in significant variability even when implementations are closely aligned within their respective vendor-specific development environments {cite:p}`Lee2019-ei`.\n",
    "\n",
    "The 2020 Reproducibility Challenge, jointly organized by the Reproducible Research and Quantitative MR ISMRM study groups, led to the creation of a large open database of standard quantitative MR phantom and human brain inversion recovery T<sub>1</sub> maps. These maps were measured using independently implemented imaging protocols on MRI scanners from three different manufacturers. All collected data, processing pipeline code, computational environment files, and analysis scripts were shared with the goal of promoting reproducible research practices, and an interactive dashboard was developed to broaden the accessibility and engagement of the resulting datasets (https://rrsg2020.dashboards.neurolibre.org). The differences in stability between independently implemented (inter-submission) and centrally shared (intra-submission) protocols observed both in phantoms and in vivo could help inform future meta-analyses of quantitative MRI metrics {cite:p}`Mancini2020-sv,Lazari2021-oy` and better guide multi-center collaborations.\n",
    "\n",
    "By providing access and analysis tools for this multi-center T<sub>1</sub> mapping dataset, we aim to provide a benchmark for future T<sub>1</sub> mapping approaches. We also hope that this dataset will inspire new acquisition, analysis, and standardization techniques that address non-physiological sources of variability in T<sub>1</sub> mapping. This could lead to more robust and reproducible quantitative MRI and ultimately better patient care.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACKNOWLEDGEMENT\n",
    "\n",
    "<p style=\"text-align:justify;\">\n",
    "The conception of this collaborative reproducibility challenge originated from discussions with experts, including Paul Tofts, Joëlle Barral, and Ilana Leppert, who provided valuable insights. Additionally, Kathryn Keenan, Zydrunas Gimbutas, and Andrew Dienstfrey from NIST provided their code to generate the ROI template for the ISMRM/NIST phantom. Dylan Roskams-Edris and Gabriel Pelletier from the Tanenbaum Open Science Institute (TOSI) offered valuable insights and guidance related to data ethics and data sharing in the context of this international multi-center conference challenge. The 2020 RRSG study group committee members who launched the challenge, Martin Uecker, Florian Knoll, Nikola Stikov, Maria Eugenia Caligiuri, and Daniel Gallichan, as well as the 2020 qMRSG committee members, Kathryn Keenan, Diego Hernando, Xavier Golay, Annie Yuxin Zhang, and Jeff Gunter, also played an essential role in making this challenge possible. We would also like to thank the Canadian Open Neuroscience Platform (CONP), the Quebec Bioimaging Network (QBIN), and the Montreal Heart Institute Foundation for their support in creating the NeuroLibre preprint. Finally, we extend our thanks to all the volunteers and individuals who helped with the scanning at each imaging site.\n",
    "\n",
    "The authors thank the ISMRM Reproducible Research Study Group for conducting a code review of the code (Version 1) supplied in the Data Availability Statement. The scope of the code review covered only the code’s ease of download, quality of documentation, and ability to run, but did not consider scientific accuracy or code efficiency.\n",
    "\n",
    "Lastly, we acknowledge use of ChatGPT (v3), a generative language model, for accelerating manuscript preparation. The co-first authors employed ChatGPT in the initial draft for transforming bullet point sentences into paragraphs, proofreading for typos, and refining the academic tone. ChatGPT served exclusively as a writing aid, and was not used to create or interpret results.\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA AVAILABILITY STATEMENT\n",
    "\n",
    "An interactive NeuroLibre preprint of this manuscript is available at https://preprint.neurolibre.org/10.55458/neurolibre.00014/. All imaging data submitted to the challenge, dataset details, registered ROI maps, and processed T<sub>1</sub> maps are hosted on OSF https://osf.io/ywc9g/. The dataset submissions and quality assurance were handled through GitHub issues in this repository https://github.com/rrsg2020/data_submission (commit: 9d7eff1). Note that accepted submissions are closed issues, and that the GitHub branches associated with the issue numbers contain the Dockerfile and Jupyter Notebook scripts that reproduce these preliminary quality assurance results and can be run in a browser using MyBinder. The ROI registration scripts for the phantoms and the T<sub>1</sub> fitting pipeline to process all datasets are hosted in this GitHub repository, https://github.com/rrsg2020/t1_fitting_pipeline (commit: 3497a4e). All the analyses of the datasets were done using Jupyter Notebooks and are available in this repository, https://github.com/rrsg2020/analysis (commit: 8d38644), which also contains a Dockerfile to reproduce the environment using a tool like MyBinder. A dashboard was developed to explore the datasets information and results in a browser, which is accessible here, https://rrsg2020.dashboards.neurolibre.org, and the code is also available on GitHub: https://github.com/rrsg2020/rrsg2020-dashboard (commit: 6ee9321). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[^rrsg2020-challenge]: [ISMRM blog post announcingn the RRRSG challenge](https://blog.ismrm.org/2019/12/12/reproducibility-challenge-2020-join-the-reproducible-research-and-quantitative-mr-study-groups-in-their-efforts-to-standardize-t1-mapping/)\n",
    "\n",
    "[^nist-website]: The [website](https://collaborate.nist.gov/mriphantoms/bin/view/MriPhantoms/SimpleImagingInstructions) provided to the researchers has since been removed from the NIST website.\n",
    "\n",
    "[^phantom-reference]: [Manufacturer's reference for the phantom](https://qmri.com/cmri-product-resources/#premium-system-resources)\n",
    "\n",
    "[^informed-consent]: This [website](https://www.uu.nl/en/research/research-data-management/guides/informed-consent-for-data-sharing) was provided as a resource to the participants for best practices to obtain informed consent for data sharing.\n",
    "\n",
    "[^issue-five]: [rrsg2020/data_submission issue #5](https://github.com/rrsg2020/data_submission/issues/5)\n",
    "\n",
    "[^their-paper]: [http://www-mrsrl.stanford.edu/~jbarral/t1map.html](http://www-mrsrl.stanford.edu/~jbarral/t1map.html)\n",
    "\n",
    "[^residual]: [ https://github.com/qMRLab/qMRLab/blob/master/src/Models_Functions/IRfun/rdNlsPr.m#L118-L129\n",
    "]( https://github.com/qMRLab/qMRLab/blob/master/src/Models_Functions/IRfun/rdNlsPr.m#L118-L129)\n",
    "\n",
    "[^public-repo]: [https://github.com/rrsg2020/analysis](https://github.com/rrsg2020/analysis)\n",
    "\n",
    "[^submission-review]: Submissions were reviewed by MB and AK. Submission guidelines (https://github.com/rrsg2020/data_submission/blob/master/README.md) and a GitHub issue checklist (https://github.com/rrsg2020/data_submission/blob/master/.github/ISSUE_TEMPLATE/data-submission-request.md) were checked. Lastly, the submitted data was passed to the T<sub>1</sub> processing pipeline and verified for quality and expected values. Feedback was sent to the authors if their submission did not adhere to the requested guidelines, or if issues with the submitted datasets were found and if possible, corrected (e.g., scaling issues between inversion time data points).\n",
    "\n",
    "[^three-t]: Strictly speaking, not all manufacturers operate at 3.0 T. Even though this is the field strength advertised by the system manufacturers, there is some deviation in actual field strength between vendors. The actual center frequencies are typically reported in the DICOM files, and these were shared for most datasets and are available in our OSF.io repository (https://osf.io/ywc9g/). From these datasets, the center frequencies imply participants that used GE and Philips scanners were at 3.0T (`~`127.7 MHz), whereas participants that used Siemens scanners were at 2.89T (`~`123.2 MHz). For simplicity, we will always refer to the field strength in this article as 3T.\n",
    "\n",
    "[^my-binder]: https://mybinder.org/v2/gh/rrsg2020/analysis/master?filepath=analysis\n",
    "\n",
    "[^nonlinear]: The T<sub>1</sub> values vs temperature tables reported by the phantom manufacturer did not always exhibit a linear relationship. We explored the use of spline fitting on the original data and quadratic fitting on the log-log representation of the data, Both methods yielded good results, and we opted to use the latter in our analyses. The code is found [here](https://github.com/rrsg2020/analysis/blob/master/src/nist.py), and a Jupyter Notebook used in temperature interpolation development is [here](https://github.com/rrsg2020/analysis/blob/master/temperature_correction.ipynb).\n",
    "\n",
    "[^phantom-version]: Only T<sub>1</sub> maps measured using phantom version 1 were included in this inter-submission COV, as including both sets would have increased the COV due to the differences in reference T<sub>1</sub> values. There were seven research groups that used version 1, and six that used version 2.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
